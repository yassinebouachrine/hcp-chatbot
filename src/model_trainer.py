# import torch
# from transformers import (
#     AutoTokenizer, AutoModelForCausalLM, 
#     TrainingArguments, Trainer, 
#     DataCollatorForLanguageModeling
# )
# from datasets import Dataset
# import json
# import os
# import time
# import psutil
# import gc

# class HCPChatbotTrainer:
#     def __init__(self, config):
#         self.config = config
#         self.tokenizer = None
#         self.model = None
#         self.hf_token = getattr(config, 'HF_TOKEN', None) or os.getenv('HUGGING_FACE_TOKEN')
#         self.setup_torch_optimizations()

#     def setup_torch_optimizations(self):
#         """Configuration optimale pour PyTorch"""
#         torch.set_num_threads(8)
#         if not torch.cuda.is_available():
#             try:
#                 torch.backends.mkl.enabled = True
#             except:
#                 pass
        
#         print(f"Threads PyTorch: {torch.get_num_threads()}")
#         print(f"RAM disponible: {psutil.virtual_memory().available / (1024**3):.1f} GB")

#     def initialize_model(self):
#         """Initialisation du mod√®le et tokenizer avec support nouvelle structure"""
#         print(f"Initialisation du mod√®le {self.config.BASE_MODEL}...")
        
#         # Tokenizer
#         self.tokenizer = AutoTokenizer.from_pretrained(
#             self.config.BASE_MODEL,
#             padding_side="left",
#             token=self.hf_token
#         )
        
#         if self.tokenizer.pad_token is None:
#             self.tokenizer.pad_token = self.tokenizer.eos_token
        
#         # NOUVEAU: Tokens sp√©ciaux adapt√©s √† la nouvelle structure
#         special_tokens = []
#         required_tokens = [
#             "<|user|>", "<|assistant|>", 
#             "<|territory|>", "<|context|>", 
#             "<|indicator|>", "<|genre|>"  # Nouveaux tokens pour la structure
#         ]
        
#         for token in required_tokens:
#             if token not in self.tokenizer.get_vocab():
#                 special_tokens.append(token)
        
#         if special_tokens:
#             self.tokenizer.add_special_tokens({"additional_special_tokens": special_tokens})
#             print(f"Tokens sp√©ciaux ajout√©s: {special_tokens}")

#         # Mod√®le
#         self.model = AutoModelForCausalLM.from_pretrained(
#             self.config.BASE_MODEL,
#             torch_dtype=torch.float32,
#             low_cpu_mem_usage=True,
#             token=self.hf_token
#         )
        
#         if special_tokens:
#             self.model.resize_token_embeddings(len(self.tokenizer))
#             print("Embeddings redimensionn√©s pour les nouveaux tokens")
        
#         print(f"Mod√®le {self.config.BASE_MODEL} initialis√© avec succ√®s")
#         print(f"Param√®tres du mod√®le: {self.model.num_parameters():,}")

#     def format_dialogue_for_training(self, data_item):
#         """Formate les dialogues pour l'entra√Ænement avec contexte enrichi"""
        
#         # Si c'est d√©j√† une conversation format√©e
#         if 'conversation' in data_item and data_item['conversation']:
#             return data_item['conversation']
        
#         # NOUVEAU: Formatage enrichi avec m√©tadonn√©es de la nouvelle structure
#         question = data_item.get('input_text', data_item.get('question', ''))
#         answer = data_item.get('target_text', data_item.get('answer', ''))
        
#         # Contexte enrichi bas√© sur la nouvelle structure
#         territory = data_item.get('territory', data_item.get('original_territory', ''))
#         indicator = data_item.get('variable', data_item.get('indicator', ''))
#         genre = data_item.get('sexe', data_item.get('genre', ''))
        
#         # Format conditionnel selon les m√©tadonn√©es disponibles
#         formatted_parts = []
        
#         if territory and territory != 'Territoire non sp√©cifi√©':
#             formatted_parts.append(f"<|territory|>{territory}")
        
#         if indicator and indicator != 'indicateur d√©mographique':
#             formatted_parts.append(f"<|indicator|>{indicator}")
            
#         if genre and genre not in ['non sp√©cifi√©', '']:
#             formatted_parts.append(f"<|genre|>{genre}")
        
#         # Construction du dialogue final
#         context_prefix = "".join(formatted_parts)
#         formatted = f"{context_prefix}<|user|>{question}<|assistant|>{answer}<|endoftext|>"
        
#         return formatted

#     def tokenize_data(self, examples):
#         """Tokenisation des donn√©es avec support du nouveau format"""
#         conversations = []
        
#         # NOUVEAU: G√©rer les deux formats de donn√©es
#         for i in range(len(examples['input_text'])):
#             data_item = {
#                 'input_text': examples['input_text'][i],
#                 'target_text': examples['target_text'][i],
#                 'territory': examples.get('territory', [''])[i] if 'territory' in examples else '',
#                 'variable': examples.get('variable', [''])[i] if 'variable' in examples else '',
#                 'sexe': examples.get('sexe', [''])[i] if 'sexe' in examples else '',
#             }
            
#             formatted_conversation = self.format_dialogue_for_training(data_item)
#             conversations.append(formatted_conversation)
        
#         # Tokenisation
#         inputs = self.tokenizer(
#             conversations,
#             truncation=True,
#             padding='max_length',
#             max_length=self.config.MAX_LENGTH,
#             return_tensors=None
#         )
        
#         inputs["labels"] = inputs["input_ids"].copy()
#         return inputs

#     def prepare_dataset(self, training_data):
#         """Pr√©paration du dataset avec validation renforc√©e"""
#         print(f"Pr√©paration du dataset avec {len(training_data)} √©chantillons...")
        
#         # NOUVEAU: Nettoyage sp√©cifique √† la nouvelle structure
#         cleaned_data = []
#         for item in training_data:
#             cleaned_item = {}
            
#             # Mapping des champs standards
#             field_mapping = {
#                 'input_text': ['input_text', 'question'],
#                 'target_text': ['target_text', 'answer'],
#                 'territory': ['territory', 'original_territory'],
#                 'variable': ['variable', 'indicator', 'indicateur'],
#                 'sexe': ['sexe', 'genre'],
#                 'question_type': ['question_type'],
#                 'data_source': ['data_source']
#             }
            
#             for target_field, source_fields in field_mapping.items():
#                 value = None
#                 for source_field in source_fields:
#                     if source_field in item and item[source_field]:
#                         value = item[source_field]
#                         break
                
#                 # Nettoyage des valeurs
#                 if isinstance(value, (list, dict, tuple)):
#                     cleaned_item[target_field] = str(value) if value else ""
#                 elif value is None:
#                     cleaned_item[target_field] = ""
#                 else:
#                     cleaned_item[target_field] = str(value).strip()
            
#             # Validation des champs obligatoires
#             if cleaned_item.get('input_text') and cleaned_item.get('target_text'):
#                 cleaned_data.append(cleaned_item)
        
#         print(f"  ‚úì {len(cleaned_data)} √©chantillons valides apr√®s nettoyage")
        
#         if not cleaned_data:
#             raise ValueError("Aucune donn√©e valide apr√®s nettoyage!")
        
#         # Cr√©ation du dataset
#         dataset = Dataset.from_list(cleaned_data)
        
#         # Tokenisation
#         tokenized_dataset = dataset.map(
#             self.tokenize_data,
#             batched=True,
#             num_proc=min(4, len(cleaned_data) // 100 + 1),
#             remove_columns=dataset.column_names,
#             desc="Tokenisation des donn√©es avec nouveau format"
#         )
        
#         print(f"Dataset tokenis√©: {len(tokenized_dataset)} √©chantillons")
#         return tokenized_dataset

#     def create_training_subset(self, training_data, max_samples=5000, balanced=True):
#         """Cr√©ation d'un sous-ensemble √©quilibr√© pour l'entra√Ænement"""
#         if len(training_data) <= max_samples:
#             return training_data
        
#         print(f"Cr√©ation d'un sous-ensemble de {max_samples} √©chantillons √† partir de {len(training_data)}")
        
#         if not balanced:
#             import random
#             random.shuffle(training_data)
#             return training_data[:max_samples]
        
#         # NOUVEAU: √âquilibrage bas√© sur la nouvelle structure
#         grouping_keys = ['territory', 'question_type', 'sexe']
#         groups = {}
        
#         for item in training_data:
#             # Cr√©er une cl√© de groupe bas√©e sur les m√©tadonn√©es disponibles
#             group_key_parts = []
#             for key in grouping_keys:
#                 value = item.get(key, 'unknown')
#                 if value and value not in ['non sp√©cifi√©', 'Territoire non sp√©cifi√©', '']:
#                     group_key_parts.append(f"{key}:{value}")
            
#             group_key = "_".join(group_key_parts) if group_key_parts else "general"
            
#             if group_key not in groups:
#                 groups[group_key] = []
#             groups[group_key].append(item)
        
#         # Distribution √©quilibr√©e
#         samples_per_group = max(1, max_samples // len(groups))
#         remaining_samples = max_samples
#         subset = []
        
#         for group_items in groups.values():
#             if remaining_samples <= 0:
#                 break
#             take = min(samples_per_group, len(group_items), remaining_samples)
#             subset.extend(group_items[:take])
#             remaining_samples -= take
        
#         # Statistiques du sous-ensemble
#         subset_stats = {
#             'territories': set(),
#             'question_types': {},
#             'data_sources': {}
#         }
        
#         for item in subset:
#             subset_stats['territories'].add(item.get('territory', 'Unknown'))
            
#             q_type = item.get('question_type', 'unknown')
#             subset_stats['question_types'][q_type] = subset_stats['question_types'].get(q_type, 0) + 1
            
#             source = item.get('data_source', 'unknown')
#             subset_stats['data_sources'][source] = subset_stats['data_sources'].get(source, 0) + 1
        
#         print(f"  Sous-ensemble cr√©√©: {len(subset)} √©chantillons")
#         print(f"  {len(subset_stats['territories'])} territoires repr√©sent√©s")
#         print(f"  {len(subset_stats['question_types'])} types de questions")
#         print(f"  Sources: {dict(list(subset_stats['data_sources'].items()))}")
        
#         return subset

#     def estimate_training_time(self, num_samples):
#         """Estimation du temps d'entra√Ænement"""
#         # Facteurs d'ajustement
#         base_samples_per_second = 1.5
        
#         if self.config.MAX_LENGTH > 512:
#             base_samples_per_second *= 0.8
#         if self.config.BATCH_SIZE > 2:
#             base_samples_per_second *= 1.2
#         if hasattr(self.config, 'NEW_DATA_STRUCTURE'):
#             # La nouvelle structure peut √™tre l√©g√®rement plus lente √† traiter
#             base_samples_per_second *= 0.9
        
#         total_samples = num_samples * self.config.NUM_EPOCHS
#         estimated_time = total_samples / base_samples_per_second / 60
        
#         print(f"Temps d'entra√Ænement estim√©: {estimated_time:.1f} minutes ({estimated_time/60:.1f} heures)")
        
#         recommend_subset = False
#         if estimated_time > 120:
#             print("‚ö†Ô∏è Attention: L'entra√Ænement pourrait √™tre long!")
#             print("Recommandations d'optimisation:")
#             print("- R√©duire NUM_EPOCHS (2-3 epochs suffisent souvent)")
#             print("- R√©duire MAX_LENGTH si vos dialogues sont courts")
#             print("- Augmenter BATCH_SIZE si la RAM le permet")
#             print("- Utiliser create_training_subset() pour un test rapide")
            
#             if estimated_time > 360:
#                 print("üö® RECOMMANDATION FORTE: Test avec un sous-ensemble d'abord!")
#                 recommend_subset = True
        
#         return estimated_time, recommend_subset

#     def create_validation_split(self, training_data, val_ratio=0.1):
#         """Cr√©ation du split validation avec √©quilibrage territorial"""
#         if val_ratio <= 0:
#             return training_data, []
        
#         # NOUVEAU: Split √©quilibr√© par territoire ET type de question
#         territory_type_groups = {}
        
#         for i, item in enumerate(training_data):
#             territory = item.get('territory', 'Unknown')
#             q_type = item.get('question_type', 'unknown')
#             key = f"{territory}_{q_type}"
            
#             if key not in territory_type_groups:
#                 territory_type_groups[key] = []
#             territory_type_groups[key].append(i)
        
#         train_indices = []
#         val_indices = []
        
#         for indices in territory_type_groups.values():
#             val_size = max(1, int(len(indices) * val_ratio))
#             val_indices.extend(indices[:val_size])
#             train_indices.extend(indices[val_size:])
        
#         train_data = [training_data[i] for i in train_indices]
#         val_data = [training_data[i] for i in val_indices]
        
#         print(f"Split validation √©quilibr√©: {len(train_data)} train, {len(val_data)} validation")
#         return train_data, val_data

#     def calculate_compatible_steps(self, dataset_size, batch_size, num_epochs, use_validation=False):
#         """Calcul des steps compatibles pour √©viter les erreurs"""
#         gradient_accumulation_steps = getattr(self.config, 'GRADIENT_ACCUMULATION_STEPS', 1)
        
#         # Steps par √©poque
#         steps_per_epoch = dataset_size // (batch_size * gradient_accumulation_steps)
#         if dataset_size % (batch_size * gradient_accumulation_steps) != 0:
#             steps_per_epoch += 1
        
#         # Total steps
#         total_steps = steps_per_epoch * num_epochs
        
#         if use_validation:
#             # Eval steps compatible
#             target_eval_ratio = 0.2
#             target_eval_steps = max(50, int(steps_per_epoch * target_eval_ratio))
            
#             eval_steps = target_eval_steps
#             while steps_per_epoch % eval_steps != 0 and eval_steps > 10:
#                 eval_steps -= 1
            
#             if eval_steps <= 10:
#                 eval_steps = steps_per_epoch
            
#             save_steps = eval_steps
#         else:
#             eval_steps = None
#             save_steps = min(steps_per_epoch, 500)
        
#         logging_steps = max(10, min(steps_per_epoch // 10, 100))
        
#         print(f"Configuration des steps pour nouvelle structure:")
#         print(f"   - Dataset: {dataset_size} √©chantillons")
#         print(f"   - Batch effectif: {batch_size * gradient_accumulation_steps}")
#         print(f"   - Steps/√©poque: {steps_per_epoch}")
#         print(f"   - Total steps: {total_steps}")
        
#         return {
#             'total_steps': total_steps,
#             'steps_per_epoch': steps_per_epoch,
#             'logging_steps': logging_steps,
#             'save_steps': save_steps,
#             'eval_steps': eval_steps
#         }

#     def train_model(self, training_data, use_validation=False, use_subset=False, max_subset_size=5000):
#         """Entra√Ænement principal avec support de la nouvelle structure"""
#         start_time = time.time()
        
#         if self.model is None:
#             self.initialize_model()
        
#         original_size = len(training_data)
        
#         # NOUVEAU: Validation de la structure des donn√©es
#         self._validate_training_data_structure(training_data)
        
#         # Estimation et recommandations
#         estimated_time, recommend_subset = self.estimate_training_time(len(training_data))
        
#         if use_subset or (recommend_subset and len(training_data) > max_subset_size):
#             if not use_subset and recommend_subset:
#                 print(f"\nüí° RECOMMANDATION: Test avec {max_subset_size} √©chantillons")
#                 response = input("Continuer avec un sous-ensemble ? (y/N): ").lower().strip()
#                 if response in ['y', 'yes', 'oui', 'o']:
#                     use_subset = True
            
#             if use_subset:
#                 training_data = self.create_training_subset(training_data, max_subset_size, balanced=True)
#                 print(f"‚úÖ Sous-ensemble: {len(training_data)} √©chantillons (vs {original_size} originaux)")
        
#         # Split validation
#         if use_validation:
#             train_data, val_data = self.create_validation_split(training_data, 0.1)
#         else:
#             train_data = training_data
#             val_data = []
        
#         # Pr√©paration des datasets
#         train_dataset = self.prepare_dataset(train_data)
#         val_dataset = self.prepare_dataset(val_data) if val_data else None
        
#         # Cr√©ation du r√©pertoire mod√®le
#         os.makedirs(self.config.MODEL_PATH, exist_ok=True)
        
#         # Configuration des steps
#         steps_config = self.calculate_compatible_steps(
#             len(train_dataset), 
#             self.config.BATCH_SIZE, 
#             self.config.NUM_EPOCHS,
#             use_validation and val_dataset is not None
#         )
        
#         # Arguments d'entra√Ænement
#         training_args = TrainingArguments(
#             output_dir=self.config.MODEL_PATH,
#             overwrite_output_dir=True,
            
#             # √âpoques et steps
#             num_train_epochs=self.config.NUM_EPOCHS,
#             max_steps=-1,
            
#             # Batch et apprentissage
#             per_device_train_batch_size=self.config.BATCH_SIZE,
#             gradient_accumulation_steps=getattr(self.config, 'GRADIENT_ACCUMULATION_STEPS', 1),
#             learning_rate=self.config.LEARNING_RATE,
#             weight_decay=getattr(self.config, 'WEIGHT_DECAY', 0.01),
            
#             # Dataloaders
#             dataloader_num_workers=min(4, getattr(self.config, 'DATALOADER_NUM_WORKERS', 2)),
#             dataloader_pin_memory=False,
#             remove_unused_columns=False,
            
#             # Logging et sauvegarde
#             logging_steps=steps_config['logging_steps'],
#             save_steps=steps_config['save_steps'],
#             save_total_limit=2,
            
#             # √âvaluation
#             evaluation_strategy="steps" if steps_config['eval_steps'] else "no",
#             eval_steps=steps_config['eval_steps'],
            
#             # Optimisations
#             prediction_loss_only=True,
#             report_to=None,
#             load_best_model_at_end=bool(steps_config['eval_steps']),
#             metric_for_best_model="eval_loss" if steps_config['eval_steps'] else None,
#             gradient_checkpointing=True,
#             bf16=False,
#             fp16=False,
#         )
        
#         # Data collator
#         data_collator = DataCollatorForLanguageModeling(
#             tokenizer=self.tokenizer,
#             mlm=False,
#         )
        
#         # Trainer
#         trainer = Trainer(
#             model=self.model,
#             args=training_args,
#             train_dataset=train_dataset,
#             eval_dataset=val_dataset,
#             data_collator=data_collator,
#         )
        
#         # Entra√Ænement
#         print(f"\nüöÄ D√âBUT DE L'ENTRA√éNEMENT")
#         print(f"Structure: {'Nouvelle (qa_pairs)' if any(item.get('data_source') == 'nouvelle_structure' for item in training_data) else 'Legacy'}")
#         print(f"Configuration: {self.config.NUM_EPOCHS} √©poques exactes")
        
#         try:
#             memory_before = psutil.virtual_memory()
#             print(f"RAM avant: {memory_before.percent:.1f}%")
            
#             gc.collect()
#             trainer.train()
            
#             # Sauvegarde
#             print("üíæ Sauvegarde du mod√®le...")
#             trainer.save_model()
#             self.tokenizer.save_pretrained(self.config.MODEL_PATH)
            
#             training_time = (time.time() - start_time) / 60
            
#             # M√©tadonn√©es enrichies
#             self.save_enhanced_metadata(training_data, training_time, val_data, original_size)
            
#             print(f"\n‚úÖ ENTRA√éNEMENT TERMIN√â EN {training_time:.1f} minutes!")
#             print(f"üìÅ Mod√®le sauv√©: {self.config.MODEL_PATH}")
            
#             self.print_enhanced_training_summary(training_data, training_time, original_size)
            
#         except Exception as e:
#             print(f"‚ùå Erreur pendant l'entra√Ænement: {e}")
#             raise
#         finally:
#             gc.collect()

#     def _validate_training_data_structure(self, training_data):
#         """Valide la structure des donn√©es d'entra√Ænement"""
#         if not training_data:
#             raise ValueError("Aucune donn√©e d'entra√Ænement fournie!")
        
#         print("üîç Validation de la structure des donn√©es...")
        
#         required_fields = ['question', 'answer']
#         optional_fields = ['territory', 'question_type', 'variable', 'sexe', 'data_source']
        
#         issues = []
#         structure_stats = {
#             'total_items': len(training_data),
#             'valid_items': 0,
#             'missing_fields': {},
#             'data_sources': {},
#             'has_new_structure': False,
#             'has_legacy_structure': False
#         }
        
#         for i, item in enumerate(training_data[:100]):  # V√©rifier les 100 premiers
#             if not isinstance(item, dict):
#                 issues.append(f"Item {i}: n'est pas un dictionnaire")
#                 continue
            
#             # V√©rifier les champs requis
#             has_question = any(field in item and item[field] for field in ['question', 'input_text'])
#             has_answer = any(field in item and item[field] for field in ['answer', 'target_text'])
            
#             if not (has_question and has_answer):
#                 for field in required_fields:
#                     if not any(alt in item and item[alt] for alt in [field, f"input_{field}", f"target_{field}"]):
#                         structure_stats['missing_fields'][field] = structure_stats['missing_fields'].get(field, 0) + 1
#                 continue
            
#             structure_stats['valid_items'] += 1
            
#             # D√©tecter le type de structure
#             if item.get('data_source') == 'nouvelle_structure':
#                 structure_stats['has_new_structure'] = True
#             elif item.get('data_source') == 'legacy_structure':
#                 structure_stats['has_legacy_structure'] = True
            
#             # Statistiques des sources
#             source = item.get('data_source', 'unknown')
#             structure_stats['data_sources'][source] = structure_stats['data_sources'].get(source, 0) + 1
        
#         # Rapport de validation
#         print(f"   ‚úì Items valides: {structure_stats['valid_items']}/{structure_stats['total_items']}")
        
#         if structure_stats['has_new_structure']:
#             print("   ‚úì Structure moderne d√©tect√©e (qa_pairs)")
#         if structure_stats['has_legacy_structure']:
#             print("   ‚úì Structure legacy d√©tect√©e")
        
#         if structure_stats['data_sources']:
#             print("   üìä Sources de donn√©es:")
#             for source, count in structure_stats['data_sources'].items():
#                 print(f"      - {source}: {count}")
        
#         if issues:
#             print(f"   ‚ö†Ô∏è {len(issues)} probl√®mes d√©tect√©s (premiers items)")
#             for issue in issues[:5]:
#                 print(f"      - {issue}")
        
#         if structure_stats['valid_items'] == 0:
#             raise ValueError("Aucun item valide trouv√© dans les donn√©es d'entra√Ænement!")
        
#         return structure_stats

#     def save_enhanced_metadata(self, training_data, training_time, val_data=None, original_size=None):
#         """Sauvegarde des m√©tadonn√©es enrichies pour la nouvelle structure"""
        
#         # Analyse des territoires
#         territories = {}
#         question_types = {}
#         indicators_found = set()
#         data_sources = {}
#         gender_distribution = {}
        
#         for item in training_data:
#             # Territoires
#             territory = item.get('territory', item.get('original_territory', 'Unknown'))
#             territories[territory] = territories.get(territory, 0) + 1
            
#             # Types de questions
#             qtype = item.get('question_type', 'unknown')
#             question_types[qtype] = question_types.get(qtype, 0) + 1
            
#             # Indicateurs
#             if 'indicators' in item and isinstance(item['indicators'], dict):
#                 indicators_found.update(item['indicators'].keys())
            
#             variable = item.get('variable', item.get('indicateur'))
#             if variable:
#                 indicators_found.add(variable)
            
#             # Sources de donn√©es
#             source = item.get('data_source', 'unknown')
#             data_sources[source] = data_sources.get(source, 0) + 1
            
#             # Distribution de genre
#             gender = item.get('sexe', item.get('genre', 'non sp√©cifi√©'))
#             gender_distribution[gender] = gender_distribution.get(gender, 0) + 1
        
#         # M√©tadonn√©es compl√®tes
#         metadata = {
#             'model_info': {
#                 'base_model': getattr(self.config, 'BASE_MODEL', 'unknown'),
#                 'model_type': 'enhanced_dialogue_chatbot',
#                 'training_format': 'conversational_with_context',
#                 'structure_support': {
#                     'nouvelle_structure': any(item.get('data_source') == 'nouvelle_structure' for item in training_data),
#                     'legacy_structure': any(item.get('data_source') == 'legacy_structure' for item in training_data)
#                 }
#             },
            
#             'training_config': {
#                 'num_samples_used': len(training_data),
#                 'num_samples_original': original_size or len(training_data),
#                 'validation_samples': len(val_data) if val_data else 0,
#                 'max_length': self.config.MAX_LENGTH,
#                 'num_epochs': self.config.NUM_EPOCHS,
#                 'learning_rate': self.config.LEARNING_RATE,
#                 'batch_size': self.config.BATCH_SIZE,
#                 'gradient_accumulation_steps': getattr(self.config, 'GRADIENT_ACCUMULATION_STEPS', 1),
#                 'subset_used': len(training_data) != (original_size or len(training_data))
#             },
            
#             'data_analysis': {
#                 'territories': {
#                     'count': len(territories),
#                     'distribution': dict(sorted(territories.items(), key=lambda x: x[1], reverse=True)[:15])
#                 },
#                 'question_types': {
#                     'count': len(question_types),
#                     'distribution': question_types
#                 },
#                 'indicators_available': sorted(list(indicators_found)),
#                 'data_sources': data_sources,
#                 'gender_distribution': gender_distribution
#             },
            
#             'performance': {
#                 'training_time_minutes': round(training_time, 2),
#                 'samples_per_minute': round(len(training_data) * self.config.NUM_EPOCHS / training_time, 2),
#                 'training_date': time.strftime("%Y-%m-%d %H:%M:%S"),
#                 'efficiency_score': min(100, round((len(training_data) * self.config.NUM_EPOCHS) / (training_time * 10), 1))
#             },
            
#             'hardware_info': {
#                 'cpu_count': psutil.cpu_count(),
#                 'ram_gb': round(psutil.virtual_memory().total / (1024**3), 1),
#                 'torch_threads': torch.get_num_threads(),
#                 'platform': 'CPU' if not torch.cuda.is_available() else 'GPU',
#                 'final_ram_usage': round(psutil.virtual_memory().percent, 1)
#             },
            
#             'usage_recommendations': {
#                 'best_question_types': list(sorted(question_types.keys(), key=lambda x: question_types[x], reverse=True)[:5]),
#                 'supported_territories': list(territories.keys())[:15],
#                 'optimal_query_format': "Mentionnez le territoire et soyez sp√©cifique",
#                 'context_tokens_supported': [
#                     "<|territory|>", "<|indicator|>", "<|genre|>", 
#                     "<|user|>", "<|assistant|>"
#                 ]
#             },
            
#             'quality_metrics': {
#                 'data_coverage': {
#                     'territory_coverage': len(territories),
#                     'question_type_coverage': len(question_types),
#                     'indicator_coverage': len(indicators_found)
#                 },
#                 'data_balance': {
#                     'most_common_territory_ratio': max(territories.values()) / len(training_data) if territories else 0,
#                     'most_common_qtype_ratio': max(question_types.values()) / len(training_data) if question_types else 0
#                 }
#             }
#         }
        
#         # Sauvegarde
#         metadata_path = os.path.join(self.config.MODEL_PATH, 'enhanced_metadata.json')
#         with open(metadata_path, 'w', encoding='utf-8') as f:
#             json.dump(metadata, f, indent=2, ensure_ascii=False)
        
#         print(f"üìã M√©tadonn√©es enrichies sauv√©es: {metadata_path}")
#         return metadata

#     def print_enhanced_training_summary(self, training_data, training_time, original_size):
#         """Affiche un r√©sum√© d√©taill√© de l'entra√Ænement"""
#         print("\n" + "="*60)
#         print("üéØ R√âSUM√â D√âTAILL√â DE L'ENTRA√éNEMENT")
#         print("="*60)
        
#         # Analyse des donn√©es
#         territories = set(item.get('territory', 'Unknown') for item in training_data)
#         question_types = {}
#         data_sources = {}
#         gender_dist = {}
        
#         for item in training_data:
#             qtype = item.get('question_type', 'unknown')
#             question_types[qtype] = question_types.get(qtype, 0) + 1
            
#             source = item.get('data_source', 'unknown')
#             data_sources[source] = data_sources.get(source, 0) + 1
            
#             gender = item.get('sexe', 'non sp√©cifi√©')
#             gender_dist[gender] = gender_dist.get(gender, 0) + 1
        
#         # Informations g√©n√©rales
#         print(f"üìä DONN√âES D'ENTRA√éNEMENT:")
#         print(f"   ‚Ä¢ √âchantillons utilis√©s: {len(training_data)}")
#         if original_size and original_size != len(training_data):
#             print(f"   ‚Ä¢ √âchantillons originaux: {original_size} (sous-ensemble utilis√©)")
#         print(f"   ‚Ä¢ Territoires couverts: {len(territories)}")
#         print(f"   ‚Ä¢ Types de questions: {len(question_types)}")
#         print(f"   ‚Ä¢ √âpoques d'entra√Ænement: {self.config.NUM_EPOCHS}")
        
#         # Sources de donn√©es
#         if len(data_sources) > 1:
#             print(f"\nüîÑ SOURCES DE DONN√âES:")
#             for source, count in data_sources.items():
#                 percentage = (count / len(training_data)) * 100
#                 print(f"   ‚Ä¢ {source}: {count} ({percentage:.1f}%)")
        
#         # Performance
#         samples_total = len(training_data) * self.config.NUM_EPOCHS
#         speed = samples_total / (training_time * 60)
        
#         print(f"\n‚ö° PERFORMANCE:")
#         print(f"   ‚Ä¢ Temps total: {training_time:.1f} minutes")
#         print(f"   ‚Ä¢ Vitesse: {speed:.1f} √©chantillons/seconde")
#         print(f"   ‚Ä¢ √âchantillons trait√©s: {samples_total:,}")
        
#         # Types de questions les plus fr√©quents
#         print(f"\nüìà TYPES DE QUESTIONS PRINCIPAUX:")
#         sorted_types = sorted(question_types.items(), key=lambda x: x[1], reverse=True)
#         for qtype, count in sorted_types[:6]:
#             percentage = (count / len(training_data)) * 100
#             print(f"   ‚Ä¢ {qtype}: {count} ({percentage:.1f}%)")
        
#         # Territoires principaux
#         territory_counts = {}
#         for item in training_data:
#             terr = item.get('territory', 'Unknown')
#             territory_counts[terr] = territory_counts.get(terr, 0) + 1
        
#         print(f"\nüó∫Ô∏è TERRITOIRES PRINCIPAUX:")
#         sorted_territories = sorted(territory_counts.items(), key=lambda x: x[1], reverse=True)
#         for territory, count in sorted_territories[:6]:
#             percentage = (count / len(training_data)) * 100
#             display_name = territory if len(territory) <= 35 else territory[:32] + "..."
#             print(f"   ‚Ä¢ {display_name}: {count} ({percentage:.1f}%)")
        
#         # Distribution de genre si disponible
#         if any(g != 'non sp√©cifi√©' for g in gender_dist.keys()):
#             print(f"\n‚öß DISTRIBUTION PAR GENRE:")
#             for gender, count in sorted(gender_dist.items(), key=lambda x: x[1], reverse=True):
#                 if gender != 'non sp√©cifi√©':
#                     percentage = (count / len(training_data)) * 100
#                     print(f"   ‚Ä¢ {gender}: {count} ({percentage:.1f}%)")
        
#         # Recommandations d'utilisation
#         print(f"\nüí° RECOMMANDATIONS D'UTILISATION:")
#         print("   ‚Ä¢ Mentionnez explicitement les territoires dans vos questions")
#         print("   ‚Ä¢ Utilisez des questions sp√©cifiques sur les indicateurs d√©mographiques")
#         print("   ‚Ä¢ Le mod√®le excelle sur les donn√©es de population l√©gale/municipale")
        
#         top_qtype = sorted_types[0][0] if sorted_types else "demographic"
#         top_territory = sorted_territories[0][0] if sorted_territories else "Maroc"
#         print(f"   ‚Ä¢ Exemple optimal: 'Quelle est la population l√©gale de {top_territory} ?'")
        
#         # Tokens sp√©ciaux support√©s
#         print(f"\nüè∑Ô∏è TOKENS DE CONTEXTE SUPPORT√âS:")
#         print("   ‚Ä¢ <|territory|> : Sp√©cifier un territoire")
#         print("   ‚Ä¢ <|indicator|> : Sp√©cifier un indicateur")
#         print("   ‚Ä¢ <|genre|> : Sp√©cifier le genre")
#         print("   ‚Ä¢ <|user|> <|assistant|> : Format conversationnel")

#     def quick_test_with_new_structure(self, training_data, max_samples=500):
#         """Test rapide sp√©cialement adapt√© √† la nouvelle structure"""
#         print("\nüß™ TEST RAPIDE AVEC NOUVELLE STRUCTURE")
#         print("="*50)
        
#         # Validation du syst√®me
#         system_check = check_system_resources(min_ram_gb=2.0)
#         if not system_check['ready']:
#             print("‚ùå Syst√®me non pr√™t pour l'entra√Ænement")
#             return None
        
#         # Configuration de test optimis√©e
#         original_config = {
#             'NUM_EPOCHS': self.config.NUM_EPOCHS,
#             'BATCH_SIZE': self.config.BATCH_SIZE,
#             'MAX_LENGTH': self.config.MAX_LENGTH
#         }
        
#         # Ajustements pour test rapide
#         self.config.NUM_EPOCHS = 1
#         self.config.BATCH_SIZE = max(1, min(self.config.BATCH_SIZE, 4))
#         self.config.MAX_LENGTH = min(self.config.MAX_LENGTH, 128)
        
#         try:
#             # Cr√©er un sous-ensemble √©quilibr√©
#             subset = self.create_training_subset(training_data, max_samples, balanced=True)
            
#             print(f"üîß Configuration de test:")
#             print(f"   ‚Ä¢ √âchantillons: {len(subset)}")
#             print(f"   ‚Ä¢ √âpoques: {self.config.NUM_EPOCHS}")
#             print(f"   ‚Ä¢ Batch size: {self.config.BATCH_SIZE}")
#             print(f"   ‚Ä¢ Max length: {self.config.MAX_LENGTH}")
            
#             # Entra√Ænement de test
#             self.train_model(subset, use_validation=False, use_subset=False)
            
#             print(f"\n‚úÖ TEST RAPIDE TERMIN√â!")
#             print("Si les r√©sultats sont satisfaisants, vous pouvez maintenant:")
#             print("   ‚Ä¢ Augmenter NUM_EPOCHS (2-3)")
#             print("   ‚Ä¢ Augmenter le nombre d'√©chantillons")
#             print("   ‚Ä¢ Entra√Æner sur le dataset complet")
            
#             return True
            
#         finally:
#             # Restaurer la configuration originale
#             for key, value in original_config.items():
#                 setattr(self.config, key, value)


# def check_system_resources(min_ram_gb: float = 4.0):
#     """V√©rification des ressources syst√®me optimis√©e"""
#     memory = psutil.virtual_memory()
#     cpu_percent = psutil.cpu_percent(interval=1)
    
#     print("üîç V√©rification des ressources pour nouvelle structure:")
#     print(f"   ‚Ä¢ CPU: {psutil.cpu_count()} c≈ìurs, utilisation: {cpu_percent:.1f}%")
#     print(f"   ‚Ä¢ RAM: {memory.total / (1024**3):.1f} GB total, {memory.available / (1024**3):.1f} GB disponible")
#     print(f"   ‚Ä¢ RAM utilis√©e: {memory.percent:.1f}%")
    
#     warnings = []
#     recommendations = []
    
#     if memory.available < min_ram_gb * (1024**3):
#         warnings.append(f"Moins de {min_ram_gb}GB RAM disponible")
#         recommendations.append("Fermer applications gourmandes en m√©moire")
    
#     if cpu_percent > 80:
#         warnings.append("CPU tr√®s charg√©")
#         recommendations.append("Attendre que la charge CPU diminue")
    
#     if memory.percent > 85:
#         warnings.append("RAM tr√®s utilis√©e")
#         recommendations.append("Red√©marrer la session ou r√©duire BATCH_SIZE")
    
#     # Recommandations sp√©cifiques √† la nouvelle structure
#     if memory.available < 8 * (1024**3):
#         recommendations.append("Nouvelle structure: R√©duire MAX_LENGTH pour optimiser")
    
#     if len(recommendations) == 0:
#         recommendations.append("Syst√®me pr√™t pour entra√Æner avec nouvelle structure!")
    
#     if warnings:
#         print("‚ö†Ô∏è Avertissements:")
#         for warning in warnings:
#             print(f"      - {warning}")
    
#     print("üí° Recommandations:")
#     for rec in recommendations:
#         print(f"      - {rec}")
    
#     # Score de pr√©paration ajust√©
#     readiness_score = 100
#     if memory.available < min_ram_gb * (1024**3):
#         readiness_score -= 40
#     if cpu_percent > 80:
#         readiness_score -= 30
#     if memory.percent > 85:
#         readiness_score -= 20
    
#     print(f"\nüìä Score de pr√©paration: {readiness_score}/100")
    
#     return {
#         'ready': readiness_score >= 70,
#         'score': readiness_score,
#         'warnings': warnings,
#         'recommendations': recommendations,
#         'memory_available_gb': memory.available / (1024**3),
#         'cpu_usage_percent': cpu_percent,
#         'optimized_for_new_structure': True
#     }


# def train_with_new_structure_data(config, training_data, quick_test=False):
#     """Fonction principale pour entra√Æner avec la nouvelle structure de donn√©es"""
#     print("üöÄ ENTRA√éNEMENT AVEC NOUVELLE STRUCTURE DE DONN√âES")
#     print("="*55)
    
#     # V√©rifications initiales
#     if not training_data:
#         print("‚ùå Aucune donn√©e d'entra√Ænement fournie")
#         return None
    
#     # V√©rifier la pr√©sence de la nouvelle structure
#     new_structure_count = sum(1 for item in training_data if item.get('data_source') == 'nouvelle_structure')
#     legacy_structure_count = len(training_data) - new_structure_count
    
#     print(f"üìä Analyse de la structure:")
#     print(f"   ‚Ä¢ Nouvelle structure: {new_structure_count} √©l√©ments")
#     print(f"   ‚Ä¢ Structure legacy: {legacy_structure_count} √©l√©ments")
    
#     # Initialiser le trainer
#     trainer = HCPChatbotTrainer(config)
    
#     if quick_test:
#         print(f"\n‚ö° Mode test rapide activ√©")
#         return trainer.quick_test_with_new_structure(training_data, max_samples=1000)
#     else:
#         # Entra√Ænement complet avec recommandations adapt√©es
#         print(f"\nüéØ Mode entra√Ænement complet")
        
#         # V√©rifications syst√®me
#         system_check = check_system_resources(min_ram_gb=4.0)
#         if not system_check['ready']:
#             print("‚ö†Ô∏è Syst√®me non optimal, mais on peut continuer avec pr√©cautions")
        
#         # Estimation et recommandations
#         estimated_time, recommend_subset = trainer.estimate_training_time(len(training_data))
        
#         # Options d'entra√Ænement
#         use_subset = recommend_subset and len(training_data) > 10000
#         use_validation = len(training_data) > 1000
        
#         print(f"\n‚öôÔ∏è Configuration d'entra√Ænement recommand√©e:")
#         print(f"   ‚Ä¢ Utiliser sous-ensemble: {'Oui' if use_subset else 'Non'}")
#         print(f"   ‚Ä¢ Utiliser validation: {'Oui' if use_validation else 'Non'}")
        
#         # Lancer l'entra√Ænement
#         trainer.train_model(
#             training_data, 
#             use_validation=use_validation,
#             use_subset=use_subset,
#             max_subset_size=8000
#         )
        
#         return trainer


# if __name__ == "__main__":
#     try:
#         from config import Config
#         from data_processor import HCPDataProcessor
        
#         print("Test du trainer avec nouvelle structure...")
        
#         # Charger les donn√©es
#         processor = HCPDataProcessor(Config)
#         data = processor.load_all_data()
        
#         if not data.empty:
#             qa_pairs = processor.create_qa_pairs()
            
#             # Test rapide
#             train_with_new_structure_data(Config, qa_pairs, quick_test=True)
#         else:
#             print("‚ùå Aucune donn√©e charg√©e pour le test")
            
#     except ImportError:
#         print("‚ùå Modules requis non disponibles. Assurez-vous que config.py et data_processor.py existent.")










# import torch
# from transformers import (
#     AutoTokenizer, AutoModelForCausalLM, 
#     TrainingArguments, Trainer, 
#     DataCollatorForLanguageModeling
# )
# from datasets import Dataset
# import json
# import os
# import time
# import psutil
# import gc

# class HCPChatbotTrainer:
#     def __init__(self, config):
#         self.config = config
#         self.tokenizer = None
#         self.model = None
#         self.hf_token = getattr(config, 'HF_TOKEN', None) or os.getenv('HUGGING_FACE_TOKEN')
#         self.setup_torch_optimizations()

#     def setup_torch_optimizations(self):
#         """Configuration optimale pour PyTorch"""
#         torch.set_num_threads(8)
#         if not torch.cuda.is_available():
#             try:
#                 torch.backends.mkl.enabled = True
#             except:
#                 pass
        
#         print(f"Threads PyTorch: {torch.get_num_threads()}")
#         print(f"RAM disponible: {psutil.virtual_memory().available / (1024**3):.1f} GB")

#     def initialize_model(self):
#         """Initialisation du mod√®le et tokenizer avec support nouvelle structure HCP"""
#         print(f"Initialisation du mod√®le {self.config.BASE_MODEL}...")
        
#         # Tokenizer
#         self.tokenizer = AutoTokenizer.from_pretrained(
#             self.config.BASE_MODEL,
#             padding_side="left",
#             token=self.hf_token
#         )
        
#         if self.tokenizer.pad_token is None:
#             self.tokenizer.pad_token = self.tokenizer.eos_token
        
#         # Tokens sp√©ciaux adapt√©s √† la nouvelle structure HCP
#         special_tokens = []
#         required_tokens = [
#             "<|user|>", "<|assistant|>", 
#             "<|territory|>", "<|indicator|>", "<|genre|>",
#             "<|source|>", "<|context|>", "<|hcp|>"
#         ]
        
#         for token in required_tokens:
#             if token not in self.tokenizer.get_vocab():
#                 special_tokens.append(token)
        
#         if special_tokens:
#             self.tokenizer.add_special_tokens({"additional_special_tokens": special_tokens})
#             print(f"Tokens sp√©ciaux HCP ajout√©s: {special_tokens}")

#         # Mod√®le
#         self.model = AutoModelForCausalLM.from_pretrained(
#             self.config.BASE_MODEL,
#             torch_dtype=torch.float32,
#             low_cpu_mem_usage=True,
#             token=self.hf_token
#         )
        
#         if special_tokens:
#             self.model.resize_token_embeddings(len(self.tokenizer))
#             print("Embeddings redimensionn√©s pour les tokens HCP")
        
#         print(f"Mod√®le {self.config.BASE_MODEL} initialis√© avec succ√®s")
#         print(f"Param√®tres du mod√®le: {self.model.num_parameters():,}")

#     def format_dialogue_for_training(self, data_item):
#         """Formate les dialogues pour l'entra√Ænement avec contexte HCP enrichi"""
        
#         # Si c'est d√©j√† une conversation format√©e
#         if 'conversation' in data_item and data_item['conversation']:
#             return data_item['conversation']
        
#         # Extraction des donn√©es
#         question = data_item.get('input_text', data_item.get('question', ''))
#         answer = data_item.get('target_text', data_item.get('answer', ''))
        
#         # M√©tadonn√©es de la nouvelle structure HCP
#         territory = data_item.get('territory', data_item.get('original_territory', ''))
#         indicator = data_item.get('variable', data_item.get('indicateur', ''))
#         genre = data_item.get('sexe', data_item.get('genre', ''))
#         source = data_item.get('source_data', data_item.get('source', ''))
        
#         # Format conditionnel selon les m√©tadonn√©es disponibles
#         context_parts = []
        
#         # Toujours commencer par le contexte HCP
#         context_parts.append("<|hcp|>")
        
#         # Territoire (tr√®s important dans les donn√©es HCP)
#         if territory and territory not in ['Territoire non sp√©cifi√©', '']:
#             context_parts.append(f"<|territory|>{territory}")
        
#         # Indicateur sp√©cifique HCP
#         if indicator and indicator not in ['indicateur_demographique', 'indicateur d√©mographique', '']:
#             context_parts.append(f"<|indicator|>{indicator}")
            
#         # Genre (important pour les stats d√©mographiques)
#         if genre and genre not in ['non sp√©cifi√©', '']:
#             context_parts.append(f"<|genre|>{genre}")
            
#         # Source des donn√©es
#         if source and source not in ['unknown', '']:
#             context_parts.append(f"<|source|>{source}")
        
#         # Construction du dialogue final optimis√© pour HCP
#         context_prefix = "".join(context_parts)
#         formatted = f"{context_prefix}<|user|>{question}<|assistant|>{answer}<|endoftext|>"
        
#         return formatted

#     def tokenize_data(self, examples):
#         """Tokenisation des donn√©es avec support du nouveau format HCP"""
#         conversations = []
        
#         # G√©rer la nouvelle structure HCP
#         for i in range(len(examples['input_text'])):
#             data_item = {
#                 'input_text': examples['input_text'][i],
#                 'target_text': examples['target_text'][i],
#                 'territory': examples.get('territory', [''])[i] if 'territory' in examples else '',
#                 'variable': examples.get('variable', [''])[i] if 'variable' in examples else '',
#                 'sexe': examples.get('sexe', [''])[i] if 'sexe' in examples else '',
#                 'source_data': examples.get('source_data', [''])[i] if 'source_data' in examples else '',
#                 'question_type': examples.get('question_type', [''])[i] if 'question_type' in examples else ''
#             }
            
#             formatted_conversation = self.format_dialogue_for_training(data_item)
#             conversations.append(formatted_conversation)
        
#         # Tokenisation avec longueur adapt√©e aux donn√©es HCP
#         inputs = self.tokenizer(
#             conversations,
#             truncation=True,
#             padding='max_length',
#             max_length=self.config.MAX_LENGTH,
#             return_tensors=None
#         )
        
#         inputs["labels"] = inputs["input_ids"].copy()
#         return inputs

#     def prepare_dataset(self, training_data):
#         """Pr√©paration du dataset avec validation renforc√©e pour HCP"""
#         print(f"Pr√©paration du dataset HCP avec {len(training_data)} √©chantillons...")
        
#         # Nettoyage sp√©cifique √† la structure HCP
#         cleaned_data = []
#         for item in training_data:
#             cleaned_item = {}
            
#             # Mapping des champs sp√©cifiques HCP
#             field_mapping = {
#                 'input_text': ['input_text', 'question'],
#                 'target_text': ['target_text', 'answer'],
#                 'territory': ['territory', 'original_territory', 'territoire'],
#                 'variable': ['variable', 'indicator', 'indicateur'],
#                 'sexe': ['sexe', 'genre'],
#                 'source_data': ['source_data', 'source'],
#                 'question_type': ['question_type'],
#                 'data_source': ['data_source']
#             }
            
#             for target_field, source_fields in field_mapping.items():
#                 value = None
#                 for source_field in source_fields:
#                     if source_field in item and item[source_field]:
#                         value = item[source_field]
#                         break
                
#                 # Nettoyage et normalisation des valeurs HCP
#                 if isinstance(value, (list, dict, tuple)):
#                     cleaned_item[target_field] = str(value) if value else ""
#                 elif value is None:
#                     cleaned_item[target_field] = ""
#                 else:
#                     cleaned_item[target_field] = str(value).strip()
                    
#                 # Normalisation sp√©cifique HCP
#                 if target_field == 'sexe' and cleaned_item[target_field] in ['', 'non sp√©cifi√©']:
#                     cleaned_item[target_field] = 'ensemble'
#                 elif target_field == 'territory' and cleaned_item[target_field] == '':
#                     cleaned_item[target_field] = 'Territoire non sp√©cifi√©'
            
#             # Validation des champs obligatoires
#             if cleaned_item.get('input_text') and cleaned_item.get('target_text'):
#                 cleaned_data.append(cleaned_item)
        
#         print(f"  ‚úì {len(cleaned_data)} √©chantillons HCP valides apr√®s nettoyage")
        
#         if not cleaned_data:
#             raise ValueError("Aucune donn√©e HCP valide apr√®s nettoyage!")
        
#         # Affichage des statistiques de nettoyage
#         territory_stats = {}
#         indicator_stats = {}
#         for item in cleaned_data:
#             territory = item.get('territory', 'Unknown')
#             territory_stats[territory] = territory_stats.get(territory, 0) + 1
            
#             indicator = item.get('variable', 'Unknown')
#             indicator_stats[indicator] = indicator_stats.get(indicator, 0) + 1
        
#         print(f"  üìä Territoires: {len(territory_stats)}, Indicateurs: {len(indicator_stats)}")
        
#         # Cr√©ation du dataset
#         dataset = Dataset.from_list(cleaned_data)
        
#         # Tokenisation
#         tokenized_dataset = dataset.map(
#             self.tokenize_data,
#             batched=True,
#             num_proc=min(4, len(cleaned_data) // 100 + 1),
#             remove_columns=dataset.column_names,
#             desc="Tokenisation des donn√©es HCP avec contexte enrichi"
#         )
        
#         print(f"Dataset HCP tokenis√©: {len(tokenized_dataset)} √©chantillons")
#         return tokenized_dataset

#     def estimate_training_time(self, num_samples):
#         """Estimation du temps d'entra√Ænement pour donn√©es HCP"""
#         # Facteurs d'ajustement sp√©cifiques HCP
#         base_samples_per_second = 1.2  # Les donn√©es HCP sont plus complexes
        
#         if self.config.MAX_LENGTH > 512:
#             base_samples_per_second *= 0.7
#         if self.config.BATCH_SIZE > 2:
#             base_samples_per_second *= 1.1
        
#         # Les donn√©es HCP avec contexte enrichi sont plus lourdes
#         base_samples_per_second *= 0.8
        
#         total_samples = num_samples * self.config.NUM_EPOCHS
#         estimated_time = total_samples / base_samples_per_second / 60
        
#         print(f"Temps d'entra√Ænement HCP estim√©: {estimated_time:.1f} minutes ({estimated_time/60:.1f} heures)")
        
#         return estimated_time, False  # CORRECTION: Ne jamais recommander automatiquement un sous-ensemble

#     def create_validation_split(self, training_data, val_ratio=0.1):
#         """Cr√©ation du split validation √©quilibr√© pour HCP - VERSION CORRIG√âE"""
#         if val_ratio <= 0:
#             return training_data, []
        
#         import random
        
#         # Split √©quilibr√© par territoire ET indicateur (crit√®res HCP importants)
#         territory_indicator_groups = {}
        
#         for i, item in enumerate(training_data):
#             territory = item.get('territory', 'Unknown')
#             indicator = item.get('variable', 'unknown')
#             key = f"{territory}_{indicator}"
            
#             if key not in territory_indicator_groups:
#                 territory_indicator_groups[key] = []
#             territory_indicator_groups[key].append(i)
        
#         train_indices = []
#         val_indices = []
        
#         # CORRECTION DU BUG : M√©langer et diviser correctement chaque groupe
#         for indices in territory_indicator_groups.values():
#             # M√©langer les indices pour avoir un split al√©atoire
#             shuffled_indices = indices.copy()
#             random.shuffle(shuffled_indices)
            
#             # Calculer le nombre d'√©chantillons pour validation
#             val_size = max(1, int(len(shuffled_indices) * val_ratio))
            
#             # CORRECTION : val d'abord, puis train avec le reste
#             val_indices.extend(shuffled_indices[:val_size])
#             train_indices.extend(shuffled_indices[val_size:])  # Le reste pour train
        
#         train_data = [training_data[i] for i in train_indices]
#         val_data = [training_data[i] for i in val_indices]
        
#         print(f"Split validation HCP √©quilibr√©: {len(train_data)} train, {len(val_data)} validation")
        
#         # Validation du split
#         if len(train_data) == 0:
#             print("‚ö†Ô∏è ERREUR: Aucune donn√©e d'entra√Ænement apr√®s split!")
#             print("Fallback: utilisation d'un split simple")
            
#             # Split simple en fallback
#             total_size = len(training_data)
#             val_size = int(total_size * val_ratio)
            
#             shuffled_data = training_data.copy()
#             random.shuffle(shuffled_data)
            
#             val_data = shuffled_data[:val_size]
#             train_data = shuffled_data[val_size:]
            
#             print(f"Split simple appliqu√©: {len(train_data)} train, {len(val_data)} validation")
        
#         # Statistiques de validation HCP
#         val_territories = set(item.get('territory') for item in val_data)
#         val_indicators = set(item.get('variable') for item in val_data)
#         train_territories = set(item.get('territory') for item in train_data)
#         train_indicators = set(item.get('variable') for item in train_data)
        
#         print(f"  Train couvre: {len(train_territories)} territoires, {len(train_indicators)} indicateurs")
#         print(f"  Validation couvre: {len(val_territories)} territoires, {len(val_indicators)} indicateurs")
        
#         return train_data, val_data

#     def calculate_compatible_steps(self, dataset_size, batch_size, num_epochs, use_validation=False):
#         """Calcul des steps compatibles pour √©viter les erreurs - optimis√© HCP"""
#         gradient_accumulation_steps = getattr(self.config, 'GRADIENT_ACCUMULATION_STEPS', 1)
        
#         # Steps par √©poque
#         steps_per_epoch = dataset_size // (batch_size * gradient_accumulation_steps)
#         if dataset_size % (batch_size * gradient_accumulation_steps) != 0:
#             steps_per_epoch += 1
        
#         # Total steps
#         total_steps = steps_per_epoch * num_epochs
        
#         if use_validation:
#             # Eval steps plus fr√©quents pour donn√©es HCP (monitoring important)
#             target_eval_ratio = 0.15
#             target_eval_steps = max(30, int(steps_per_epoch * target_eval_ratio))
            
#             eval_steps = target_eval_steps
#             while steps_per_epoch % eval_steps != 0 and eval_steps > 5:
#                 eval_steps -= 1
            
#             if eval_steps <= 5:
#                 eval_steps = steps_per_epoch
            
#             save_steps = eval_steps
#         else:
#             eval_steps = None
#             save_steps = min(steps_per_epoch, 300)
        
#         # Logging plus fr√©quent pour surveiller l'entra√Ænement HCP
#         logging_steps = max(5, min(steps_per_epoch // 15, 50))
        
#         print(f"Configuration des steps pour donn√©es HCP:")
#         print(f"   - Dataset HCP: {dataset_size} √©chantillons")
#         print(f"   - Batch effectif: {batch_size * gradient_accumulation_steps}")
#         print(f"   - Steps/√©poque: {steps_per_epoch}")
#         print(f"   - Total steps: {total_steps}")
#         print(f"   - Logging steps: {logging_steps} (monitoring HCP renforc√©)")
        
#         return {
#             'total_steps': total_steps,
#             'steps_per_epoch': steps_per_epoch,
#             'logging_steps': logging_steps,
#             'save_steps': save_steps,
#             'eval_steps': eval_steps
#         }

#     def train_model(self, training_data, use_validation=False, force_use_all_data=False):
#         """Entra√Ænement principal avec support de la structure HCP - VERSION MODIFI√âE"""
#         start_time = time.time()
        
#         if self.model is None:
#             self.initialize_model()
        
#         original_size = len(training_data)
        
#         # Validation de la structure des donn√©es HCP
#         self._validate_hcp_training_data(training_data)
        
#         # CORRECTION MAJEURE: Estimation sans forcer le sous-ensemble
#         estimated_time, _ = self.estimate_training_time(len(training_data))
        
#         # CORRECTION: Afficher l'estimation mais ne pas forcer le sous-ensemble
#         if estimated_time > 90:
#             print(f"‚è∞ Estimation: {estimated_time:.1f} minutes d'entra√Ænement")
#             print("üí° Conseils d'optimisation disponibles mais entra√Ænement sur toutes les donn√©es...")
        
#         # FORCE L'UTILISATION DE TOUTES LES DONN√âES si demand√©
#         if force_use_all_data:
#             print(f"üöÄ ENTRA√éNEMENT FORC√â SUR TOUTES LES DONN√âES: {len(training_data)} √©chantillons")
        
#         # Split validation
#         if use_validation:
#             train_data, val_data = self.create_validation_split(training_data, 0.1)
#         else:
#             train_data = training_data
#             val_data = []
        
#         # Pr√©paration des datasets
#         train_dataset = self.prepare_dataset(train_data)
#         val_dataset = self.prepare_dataset(val_data) if val_data else None
        
#         # Cr√©ation du r√©pertoire mod√®le
#         os.makedirs(self.config.MODEL_PATH, exist_ok=True)
        
#         # Configuration des steps
#         steps_config = self.calculate_compatible_steps(
#             len(train_dataset), 
#             self.config.BATCH_SIZE, 
#             self.config.NUM_EPOCHS,
#             use_validation and val_dataset is not None
#         )
        
#         # Arguments d'entra√Ænement optimis√©s pour HCP
#         training_args = TrainingArguments(
#             output_dir=self.config.MODEL_PATH,
#             overwrite_output_dir=True,
            
#             # √âpoques et steps
#             num_train_epochs=self.config.NUM_EPOCHS,
#             max_steps=-1,
            
#             # Batch et apprentissage (ajust√© pour donn√©es HCP)
#             per_device_train_batch_size=self.config.BATCH_SIZE,
#             gradient_accumulation_steps=getattr(self.config, 'GRADIENT_ACCUMULATION_STEPS', 1),
#             learning_rate=self.config.LEARNING_RATE,
#             weight_decay=getattr(self.config, 'WEIGHT_DECAY', 0.01),
#             warmup_steps=max(10, steps_config['total_steps'] // 20),  # Warmup pour HCP
            
#             # Dataloaders
#             dataloader_num_workers=min(4, getattr(self.config, 'DATALOADER_NUM_WORKERS', 2)),
#             dataloader_pin_memory=False,
#             remove_unused_columns=False,
            
#             # Logging et sauvegarde (plus fr√©quent pour HCP)
#             logging_steps=steps_config['logging_steps'],
#             save_steps=steps_config['save_steps'],
#             save_total_limit=3,  # Plus de sauvegardes pour HCP
            
#             # √âvaluation
#             evaluation_strategy="steps" if steps_config['eval_steps'] else "no",
#             eval_steps=steps_config['eval_steps'],
            
#             # Optimisations
#             prediction_loss_only=True,
#             report_to=None,
#             load_best_model_at_end=bool(steps_config['eval_steps']),
#             metric_for_best_model="eval_loss" if steps_config['eval_steps'] else None,
#             gradient_checkpointing=True,
#             bf16=False,
#             fp16=False,
#         )
        
#         # Data collator
#         data_collator = DataCollatorForLanguageModeling(
#             tokenizer=self.tokenizer,
#             mlm=False,
#         )
        
#         # Trainer
#         trainer = Trainer(
#             model=self.model,
#             args=training_args,
#             train_dataset=train_dataset,
#             eval_dataset=val_dataset,
#             data_collator=data_collator,
#         )
        
#         # Entra√Ænement
#         print(f"\nüöÄ D√âBUT DE L'ENTRA√éNEMENT HCP SUR TOUTES LES DONN√âES")
#         print(f"Structure: Donn√©es HCP avec contexte enrichi")
#         print(f"Configuration: {self.config.NUM_EPOCHS} √©poques avec monitoring renforc√©")
#         print(f"√âchantillons d'entra√Ænement: {len(train_dataset):,}")
#         if val_dataset:
#             print(f"√âchantillons de validation: {len(val_dataset):,}")
        
#         try:
#             memory_before = psutil.virtual_memory()
#             print(f"RAM avant: {memory_before.percent:.1f}%")
            
#             gc.collect()
#             trainer.train()
            
#             # Sauvegarde
#             print("üíæ Sauvegarde du mod√®le HCP...")
#             trainer.save_model()
#             self.tokenizer.save_pretrained(self.config.MODEL_PATH)
            
#             training_time = (time.time() - start_time) / 60
            
#             # M√©tadonn√©es enrichies HCP
#             self.save_hcp_enhanced_metadata(train_data, training_time, val_data, original_size)
            
#             print(f"\n‚úÖ ENTRA√éNEMENT HCP TERMIN√â EN {training_time:.1f} minutes!")
#             print(f"üìÅ Mod√®le HCP sauv√©: {self.config.MODEL_PATH}")
            
#             self.print_hcp_training_summary(train_data, training_time, original_size)
            
#         except Exception as e:
#             print(f"‚ùå Erreur pendant l'entra√Ænement HCP: {e}")
#             raise
#         finally:
#             gc.collect()

#     def _validate_hcp_training_data(self, training_data):
#         """Valide la structure des donn√©es d'entra√Ænement HCP"""
#         if not training_data:
#             raise ValueError("Aucune donn√©e d'entra√Ænement HCP fournie!")
        
#         print("üîç Validation de la structure des donn√©es HCP...")
        
#         required_fields = ['question', 'answer']
#         hcp_fields = ['territory', 'variable', 'source_data', 'sexe']
        
#         issues = []
#         structure_stats = {
#             'total_items': len(training_data),
#             'valid_items': 0,
#             'missing_fields': {},
#             'hcp_coverage': {},
#             'has_new_structure': False,
#             'territory_count': 0,
#             'indicator_count': 0,
#             'sources': set()
#         }
        
#         territories = set()
#         indicators = set()
        
#         for i, item in enumerate(training_data[:100]):  # V√©rifier les 100 premiers
#             if not isinstance(item, dict):
#                 issues.append(f"Item {i}: n'est pas un dictionnaire")
#                 continue
            
#             # V√©rifier les champs requis
#             has_question = any(field in item and item[field] for field in ['question', 'input_text'])
#             has_answer = any(field in item and item[field] for field in ['answer', 'target_text'])
            
#             if not (has_question and has_answer):
#                 for field in required_fields:
#                     if not any(alt in item and item[alt] for alt in [field, f"input_{field}", f"target_{field}"]):
#                         structure_stats['missing_fields'][field] = structure_stats['missing_fields'].get(field, 0) + 1
#                 continue
            
#             structure_stats['valid_items'] += 1
            
#             # Analyser les champs HCP sp√©cifiques
#             territory = item.get('territory', item.get('original_territory', ''))
#             if territory and territory != 'Territoire non sp√©cifi√©':
#                 territories.add(territory)
#                 structure_stats['hcp_coverage']['territory'] = structure_stats['hcp_coverage'].get('territory', 0) + 1
            
#             indicator = item.get('variable', item.get('indicateur', ''))
#             if indicator and indicator not in ['indicateur_demographique', '']:
#                 indicators.add(indicator)
#                 structure_stats['hcp_coverage']['indicator'] = structure_stats['hcp_coverage'].get('indicator', 0) + 1
            
#             source = item.get('source_data', item.get('source', ''))
#             if source:
#                 structure_stats['sources'].add(source)
            
#             # D√©tecter le type de structure
#             if item.get('data_source') == 'nouvelle_structure':
#                 structure_stats['has_new_structure'] = True
        
#         structure_stats['territory_count'] = len(territories)
#         structure_stats['indicator_count'] = len(indicators)
        
#         # Rapport de validation HCP
#         print(f"   ‚úì Items HCP valides: {structure_stats['valid_items']}/{structure_stats['total_items']}")
#         print(f"   üìç Territoires d√©tect√©s: {structure_stats['territory_count']}")
#         print(f"   üìä Indicateurs d√©tect√©s: {structure_stats['indicator_count']}")
        
#         if structure_stats['sources']:
#             print(f"   üîÑ Sources HCP: {', '.join(list(structure_stats['sources'])[:5])}")
        
#         if structure_stats['has_new_structure']:
#             print("   ‚úì Structure moderne HCP d√©tect√©e")
        
#         # Afficher les territoires principaux
#         if territories:
#             top_territories = list(territories)[:5]
#             print(f"   üèõÔ∏è Territoires principaux: {', '.join(t[:30] + '...' if len(t) > 30 else t for t in top_territories)}")
        
#         # Afficher les indicateurs principaux  
#         if indicators:
#             top_indicators = list(indicators)[:5]
#             print(f"   üìà Indicateurs principaux: {', '.join(top_indicators)}")
        
#         if issues:
#             print(f"   ‚ö†Ô∏è {len(issues)} probl√®mes d√©tect√©s (premiers items)")
#             for issue in issues[:3]:
#                 print(f"      - {issue}")
        
#         if structure_stats['valid_items'] == 0:
#             raise ValueError("Aucun item HCP valide trouv√© dans les donn√©es d'entra√Ænement!")
        
#         # Validation sp√©cifique HCP
#         if structure_stats['territory_count'] < 2:
#             print("   ‚ö†Ô∏è Peu de territoires d√©tect√©s - v√©rifiez la qualit√© des donn√©es")
        
#         if structure_stats['indicator_count'] < 5:
#             print("   ‚ö†Ô∏è Peu d'indicateurs d√©tect√©s - v√©rifiez la diversit√© des donn√©es")
        
#         return structure_stats

#     def save_hcp_enhanced_metadata(self, training_data, training_time, val_data=None, original_size=None):
#         """Sauvegarde des m√©tadonn√©es enrichies sp√©cifiques HCP"""
        
#         # Analyse sp√©cifique HCP
#         territories = {}
#         indicators = {}
#         sources = {}
#         gender_distribution = {}
#         question_types = {}
        
#         for item in training_data:
#             # Territoires HCP
#             territory = item.get('territory', item.get('original_territory', 'Unknown'))
#             territories[territory] = territories.get(territory, 0) + 1
            
#             # Indicateurs HCP
#             indicator = item.get('variable', item.get('indicateur', 'unknown'))
#             indicators[indicator] = indicators.get(indicator, 0) + 1
            
#             # Sources HCP
#             source = item.get('source_data', item.get('source', 'unknown'))
#             sources[source] = sources.get(source, 0) + 1
            
#             # Distribution de genre
#             gender = item.get('sexe', item.get('genre', 'ensemble'))
#             gender_distribution[gender] = gender_distribution.get(gender, 0) + 1
            
#             # Types de questions
#             qtype = item.get('question_type', 'unknown')
#             question_types[qtype] = question_types.get(qtype, 0) + 1
        
#         # M√©tadonn√©es HCP compl√®tes
#         metadata = {
#             'model_info': {
#                 'base_model': getattr(self.config, 'BASE_MODEL', 'unknown'),
#                 'model_type': 'hcp_demographic_chatbot',
#                 'training_format': 'conversational_with_hcp_context',
#                 'hcp_structure_support': True,
#                 'specialized_domain': 'moroccan_demographics',
#                 'trained_on_full_dataset': True  # AJOUT: Marqueur pour toutes les donn√©es
#             },
            
#             'training_config': {
#                 'num_samples_used': len(training_data),
#                 'num_samples_original': original_size or len(training_data),
#                 'validation_samples': len(val_data) if val_data else 0,
#                 'max_length': self.config.MAX_LENGTH,
#                 'num_epochs': self.config.NUM_EPOCHS,
#                 'learning_rate': self.config.LEARNING_RATE,
#                 'batch_size': self.config.BATCH_SIZE,
#                 'gradient_accumulation_steps': getattr(self.config, 'GRADIENT_ACCUMULATION_STEPS', 1),
#                 'subset_used': False,  # CORRECTION: Marquer comme faux
#                 'full_dataset_training': True,  # AJOUT: Marqueur explicite
#                 'hcp_optimized': True
#             },
            
#             'hcp_data_analysis': {
#                 'territories': {
#                     'count': len(territories),
#                     'distribution': dict(sorted(territories.items(), key=lambda x: x[1], reverse=True)[:20]),
#                     'coverage_type': 'national_and_regional'
#                 },
#                 'indicators': {
#                     'count': len(indicators),
#                     'distribution': dict(sorted(indicators.items(), key=lambda x: x[1], reverse=True)[:15]),
#                     'main_categories': ['population_legale', 'population_municipale', 'demographics', 'employment', 'education']
#                 },
#                 'data_sources': {
#                     'count': len(sources),
#                     'distribution': sources,
#                     'primary_source': 'HCP_Morocco'
#                 },
#                 'gender_coverage': gender_distribution,
#                 'question_types': question_types
#             },
            
#             'performance': {
#                 'training_time_minutes': round(training_time, 2),
#                 'samples_per_minute': round(len(training_data) * self.config.NUM_EPOCHS / training_time, 2),
#                 'training_date': time.strftime("%Y-%m-%d %H:%M:%S"),
#                 'efficiency_score': min(100, round((len(training_data) * self.config.NUM_EPOCHS) / (training_time * 10), 1)),
#                 'hcp_complexity_factor': 1.2  # Les donn√©es HCP sont plus complexes
#             },
            
#             'hardware_info': {
#                 'cpu_count': psutil.cpu_count(),
#                 'ram_gb': round(psutil.virtual_memory().total / (1024**3), 1),
#                 'torch_threads': torch.get_num_threads(),
#                 'platform': 'CPU' if not torch.cuda.is_available() else 'GPU',
#                 'final_ram_usage': round(psutil.virtual_memory().percent, 1),
#                 'optimized_for_hcp': True
#             },
            
#             'usage_recommendations': {
#                 'best_territories': list(sorted(territories.keys(), key=lambda x: territories[x], reverse=True)[:10]),
#                 'best_indicators': list(sorted(indicators.keys(), key=lambda x: indicators[x], reverse=True)[:8]),
#                 'optimal_query_format': "Mentionnez territoire + indicateur sp√©cifique HCP",
#                 'context_tokens_supported': [
#                     "<|hcp|>", "<|territory|>", "<|indicator|>", "<|genre|>", 
#                     "<|source|>", "<|user|>", "<|assistant|>"
#                 ],
#                 'example_queries': [
#                     "Quelle est la population l√©gale de Casablanca ?",
#                     "Pourcentage population masculine Ensemble du territoire national ?",
#                     "Population municipale Rabat ensemble ?"
#                 ]
#             },
            
#             'quality_metrics': {
#                 'data_coverage': {
#                     'territory_coverage': len(territories),
#                     'indicator_coverage': len(indicators),
#                     'source_coverage': len(sources),
#                     'gender_coverage': len(gender_distribution)
#                 },
#                 'data_balance': {
#                     'most_common_territory_ratio': max(territories.values()) / len(training_data) if territories else 0,
#                     'most_common_indicator_ratio': max(indicators.values()) / len(training_data) if indicators else 0,
#                     'gender_balance_score': min(gender_distribution.values()) / max(gender_distribution.values()) if len(gender_distribution) > 1 else 1.0
#                 },
#                 'hcp_specialization_score': min(100, len(indicators) * len(territories) / 100)
#             }
#         }
        
#         # Sauvegarde
#         metadata_path = os.path.join(self.config.MODEL_PATH, 'hcp_enhanced_metadata.json')
#         with open(metadata_path, 'w', encoding='utf-8') as f:
#             json.dump(metadata, f, indent=2, ensure_ascii=False)
        
#         print(f"üìã M√©tadonn√©es HCP enrichies sauv√©es: {metadata_path}")
#         return metadata

#     def print_hcp_training_summary(self, training_data, training_time, original_size):
#         """Affiche un r√©sum√© d√©taill√© de l'entra√Ænement HCP"""
#         print("\n" + "="*70)
#         print("üéØ R√âSUM√â D√âTAILL√â DE L'ENTRA√éNEMENT HCP - TOUTES DONN√âES")
#         print("="*70)
        
#         # Analyse des donn√©es HCP
#         territories = {}
#         indicators = {}
#         sources = {}
#         gender_dist = {}
        
#         for item in training_data:
#             # Territoires
#             territory = item.get('territory', item.get('original_territory', 'Unknown'))
#             territories[territory] = territories.get(territory, 0) + 1
            
#             # Indicateurs
#             indicator = item.get('variable', item.get('indicateur', 'unknown'))
#             indicators[indicator] = indicators.get(indicator, 0) + 1
            
#             # Sources
#             source = item.get('source_data', item.get('source', 'unknown'))
#             sources[source] = sources.get(source, 0) + 1
            
#             # Genre
#             gender = item.get('sexe', item.get('genre', 'ensemble'))
#             gender_dist[gender] = gender_dist.get(gender, 0) + 1
        
#         # Informations g√©n√©rales HCP
#         print(f"üìä DONN√âES D'ENTRA√éNEMENT HCP (TOUTES UTILIS√âES):")
#         print(f"   ‚Ä¢ √âchantillons utilis√©s: {len(training_data):,}")
#         if original_size and original_size != len(training_data):
#             print(f"   ‚Ä¢ √âchantillons originaux: {original_size:,}")
#         else:
#             print(f"   ‚Ä¢ √âchantillons originaux: {len(training_data):,} (100% utilis√©)")
#         print(f"   ‚Ä¢ Territoires HCP couverts: {len(territories)}")
#         print(f"   ‚Ä¢ Indicateurs d√©mographiques: {len(indicators)}")
#         print(f"   ‚Ä¢ Sources de donn√©es: {len(sources)}")
#         print(f"   ‚Ä¢ √âpoques d'entra√Ænement: {self.config.NUM_EPOCHS}")
        
#         # Sources de donn√©es HCP
#         if len(sources) > 1:
#             print(f"\nüîÑ SOURCES DE DONN√âES HCP:")
#             for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
#                 percentage = (count / len(training_data)) * 100
#                 print(f"   ‚Ä¢ {source}: {count:,} ({percentage:.1f}%)")
        
#         # Performance
#         samples_total = len(training_data) * self.config.NUM_EPOCHS
#         speed = samples_total / (training_time * 60)
        
#         print(f"\n‚ö° PERFORMANCE D'ENTRA√éNEMENT:")
#         print(f"   ‚Ä¢ Temps total: {training_time:.1f} minutes")
#         print(f"   ‚Ä¢ Vitesse: {speed:.1f} √©chantillons/seconde")
#         print(f"   ‚Ä¢ √âchantillons trait√©s: {samples_total:,}")
#         print(f"   ‚Ä¢ Complexit√© HCP: √âlev√©e (contexte enrichi)")
        
#         # Indicateurs HCP principaux
#         print(f"\nüìà INDICATEURS D√âMOGRAPHIQUES PRINCIPAUX:")
#         sorted_indicators = sorted(indicators.items(), key=lambda x: x[1], reverse=True)
#         for indicator, count in sorted_indicators[:8]:
#             percentage = (count / len(training_data)) * 100
#             print(f"   ‚Ä¢ {indicator}: {count:,} ({percentage:.1f}%)")
        
#         # Territoires HCP principaux
#         print(f"\nüó∫Ô∏è TERRITOIRES HCP PRINCIPAUX:")
#         sorted_territories = sorted(territories.items(), key=lambda x: x[1], reverse=True)
#         for territory, count in sorted_territories[:8]:
#             percentage = (count / len(training_data)) * 100
#             display_name = territory if len(territory) <= 45 else territory[:42] + "..."
#             print(f"   ‚Ä¢ {display_name}: {count:,} ({percentage:.1f}%)")
        
#         # Distribution de genre HCP
#         if len(gender_dist) > 1:
#             print(f"\n‚öß DISTRIBUTION PAR GENRE (HCP):")
#             for gender, count in sorted(gender_dist.items(), key=lambda x: x[1], reverse=True):
#                 percentage = (count / len(training_data)) * 100
#                 print(f"   ‚Ä¢ {gender}: {count:,} ({percentage:.1f}%)")
        
#         # Recommandations d'utilisation HCP
#         print(f"\nüí° RECOMMANDATIONS D'UTILISATION HCP:")
#         print("   ‚Ä¢ Mentionnez explicitement les territoires dans vos questions")
#         print("   ‚Ä¢ Sp√©cifiez l'indicateur d√©mographique recherch√©")
#         print("   ‚Ä¢ Le mod√®le excelle sur les donn√©es HCP officielles")
#         print("   ‚Ä¢ Utilisez les termes exacts HCP pour de meilleurs r√©sultats")
        
#         # Exemples optimaux
#         top_territory = sorted_territories[0][0] if sorted_territories else "Ensemble du territoire national"
#         top_indicator = sorted_indicators[0][0] if sorted_indicators else "population_legale"
        
#         print(f"\nüéØ EXEMPLES DE REQU√äTES OPTIMALES:")
#         print(f"   ‚Ä¢ 'Quelle est la {top_indicator.replace('_', ' ')} de {top_territory} ?'")
#         print(f"   ‚Ä¢ 'Population l√©gale {top_territory} ensemble'")
#         print(f"   ‚Ä¢ 'Pourcentage population masculine {top_territory}'")
        
#         # Tokens sp√©ciaux HCP
#         print(f"\nüè∑Ô∏è TOKENS DE CONTEXTE HCP SUPPORT√âS:")
#         print("   ‚Ä¢ <|hcp|> : Contexte HCP g√©n√©ral")
#         print("   ‚Ä¢ <|territory|> : Sp√©cifier un territoire")
#         print("   ‚Ä¢ <|indicator|> : Sp√©cifier un indicateur d√©mographique")
#         print("   ‚Ä¢ <|genre|> : Sp√©cifier le genre (masculin/f√©minin/ensemble)")
#         print("   ‚Ä¢ <|source|> : Source des donn√©es HCP")
#         print("   ‚Ä¢ <|user|> <|assistant|> : Format conversationnel")
        
#         print(f"\n‚úÖ Mod√®le HCP entra√Æn√© sur TOUTES LES DONN√âES - Pr√™t pour production!")


# def check_system_resources_hcp(min_ram_gb: float = 4.0):
#     """V√©rification des ressources syst√®me optimis√©e pour HCP"""
#     memory = psutil.virtual_memory()
#     cpu_percent = psutil.cpu_percent(interval=1)
    
#     print("üîç V√©rification des ressources pour donn√©es HCP:")
#     print(f"   ‚Ä¢ CPU: {psutil.cpu_count()} c≈ìurs, utilisation: {cpu_percent:.1f}%")
#     print(f"   ‚Ä¢ RAM: {memory.total / (1024**3):.1f} GB total, {memory.available / (1024**3):.1f} GB disponible")
#     print(f"   ‚Ä¢ RAM utilis√©e: {memory.percent:.1f}%")
    
#     warnings = []
#     recommendations = []
    
#     if memory.available < min_ram_gb * (1024**3):
#         warnings.append(f"Moins de {min_ram_gb}GB RAM disponible")
#         recommendations.append("Fermer applications gourmandes en m√©moire")
    
#     if cpu_percent > 80:
#         warnings.append("CPU tr√®s charg√©")
#         recommendations.append("Attendre que la charge CPU diminue")
    
#     if memory.percent > 85:
#         warnings.append("RAM tr√®s utilis√©e")
#         recommendations.append("Red√©marrer la session ou r√©duire BATCH_SIZE")
    
#     # Recommandations sp√©cifiques HCP
#     if memory.available < 8 * (1024**3):
#         recommendations.append("Donn√©es HCP: R√©duire MAX_LENGTH pour optimiser")
#         recommendations.append("Donn√©es HCP: Consid√©rer un entra√Ænement par batch")
    
#     if len(recommendations) == 0:
#         recommendations.append("Syst√®me pr√™t pour entra√Æner avec toutes les donn√©es HCP!")
    
#     if warnings:
#         print("‚ö†Ô∏è Avertissements:")
#         for warning in warnings:
#             print(f"      - {warning}")
    
#     print("üí° Recommandations HCP:")
#     for rec in recommendations:
#         print(f"      - {rec}")
    
#     # Score de pr√©paration ajust√© pour HCP
#     readiness_score = 100
#     if memory.available < min_ram_gb * (1024**3):
#         readiness_score -= 25  # Moins p√©nalisant pour permettre l'entra√Ænement
#     if cpu_percent > 80:
#         readiness_score -= 20
#     if memory.percent > 85:
#         readiness_score -= 15
    
#     print(f"\nüìä Score de pr√©paration HCP: {readiness_score}/100")
    
#     return {
#         'ready': readiness_score >= 60,  # Seuil plus permissif
#         'score': readiness_score,
#         'warnings': warnings,
#         'recommendations': recommendations,
#         'memory_available_gb': memory.available / (1024**3),
#         'cpu_usage_percent': cpu_percent,
#         'optimized_for_hcp': True,
#         'hcp_ready': readiness_score >= 50  # Seuil encore plus bas
#     }


# def train_with_hcp_structure_data(config, training_data, force_full_training=True):
#     """Fonction principale pour entra√Æner avec TOUTES les donn√©es HCP - VERSION CORRIG√âE"""
#     print("üöÄ ENTRA√éNEMENT AVEC TOUTES LES DONN√âES HCP")
#     print("="*60)
    
#     # V√©rifications initiales
#     if not training_data:
#         print("‚ùå Aucune donn√©e d'entra√Ænement HCP fournie")
#         return None
    
#     print(f"üìä ANALYSE INITIALE:")
#     print(f"   ‚Ä¢ Total donn√©es disponibles: {len(training_data):,}")
    
#     # V√©rifier la pr√©sence de la structure HCP
#     hcp_structure_count = sum(1 for item in training_data if item.get('data_source') == 'nouvelle_structure')
#     legacy_structure_count = len(training_data) - hcp_structure_count
    
#     print(f"   ‚Ä¢ Structure HCP moderne: {hcp_structure_count:,} √©l√©ments")
#     print(f"   ‚Ä¢ Structure legacy: {legacy_structure_count:,} √©l√©ments")
    
#     # Analyser la richesse des donn√©es HCP
#     territories = set()
#     indicators = set()
#     sources = set()
    
#     for item in training_data[:1000]:  # √âchantillon pour analyse rapide
#         territory = item.get('territory', item.get('original_territory'))
#         if territory:
#             territories.add(territory)
        
#         indicator = item.get('variable', item.get('indicateur'))
#         if indicator:
#             indicators.add(indicator)
        
#         source = item.get('source_data', item.get('source'))
#         if source:
#             sources.add(source)
    
#     print(f"   ‚Ä¢ Territoires d√©tect√©s: {len(territories)}")
#     print(f"   ‚Ä¢ Indicateurs d√©tect√©s: {len(indicators)}")
#     print(f"   ‚Ä¢ Sources d√©tect√©es: {len(sources)}")
    
#     # Initialiser le trainer HCP
#     trainer = HCPChatbotTrainer(config)
    
#     # FORCE L'UTILISATION DE TOUTES LES DONN√âES
#     if force_full_training:
#         print(f"\nüéØ MODE ENTRA√éNEMENT COMPLET FORC√â")
#         print(f"   ‚Ä¢ TOUTES les {len(training_data):,} donn√©es seront utilis√©es")
#         print(f"   ‚Ä¢ Aucun sous-ensemble ne sera cr√©√©")
        
#         # V√©rifications syst√®me avec seuil plus permissif
#         system_check = check_system_resources_hcp(min_ram_gb=3.0)
#         if not system_check['hcp_ready']:
#             print("‚ö†Ô∏è Ressources limit√©es mais entra√Ænement autoris√©")
#             print("üí° Conseils: Surveillez la RAM et fermez d'autres applications")
#         else:
#             print("‚úÖ Syst√®me pr√™t pour l'entra√Ænement complet")
        
#         # Recommandation sur la validation
#         use_validation = len(training_data) > 1000
#         print(f"   ‚Ä¢ Validation crois√©e: {'Activ√©e' if use_validation else 'D√©sactiv√©e'}")
        
#         # Lancer l'entra√Ænement HCP avec toutes les donn√©es
#         result = trainer.train_model(
#             training_data, 
#             use_validation=use_validation,
#             force_use_all_data=True  # PARAM√àTRE FORC√â
#         )
        
#         return trainer
    
#     else:
#         print("‚ùå Mode entra√Ænement partiel non autoris√© dans cette version")
#         return None


# if __name__ == "__main__":
#     try:
#         from config import Config
#         from data_processor import HCPDataProcessor
        
#         print("D√©marrage de l'entra√Ænement HCP avec TOUTES les donn√©es...")
        
#         # Charger les donn√©es HCP
#         processor = HCPDataProcessor(Config)
#         data = processor.load_all_data()
        
#         if not data.empty:
#             qa_pairs = processor.create_qa_pairs()
#             print(f"Donn√©es HCP charg√©es: {len(qa_pairs):,} paires QA")
            
#             # Entra√Ænement complet avec toutes les donn√©es
#             trainer = train_with_hcp_structure_data(
#                 Config, 
#                 qa_pairs, 
#                 force_full_training=True  # FORCE L'UTILISATION COMPL√àTE
#             )
            
#             if trainer:
#                 print("‚úÖ Entra√Ænement termin√© avec succ√®s sur toutes les donn√©es!")
#             else:
#                 print("‚ùå Erreur lors de l'entra√Ænement")
                
#         else:
#             print("‚ùå Aucune donn√©e HCP charg√©e")
            
#     except ImportError:
#         print("‚ùå Modules requis non disponibles. Assurez-vous que config.py et data_processor.py existent.")






import os
import time
import gc
import json
import psutil
import torch
from collections import defaultdict
from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling
)


class HCPChatbotTrainer:
    """Trainer adapt√© pour la structure HCP avec corrections pour CUDA multiprocessing."""

    def __init__(self, config):
        self.config = config
        self.tokenizer = None
        self.model = None
        self.hf_token = getattr(config, 'HF_TOKEN', None) or os.getenv('HUGGING_FACE_TOKEN')
        
        # Fix pour CUDA multiprocessing
        if torch.cuda.is_available():
            # D√©sactiver le multiprocessing pour √©viter les erreurs CUDA
            os.environ['TOKENIZERS_PARALLELISM'] = 'false'
            torch.multiprocessing.set_start_method('spawn', force=True)
        
        self.setup_torch_optimizations()

    def setup_torch_optimizations(self):
        torch.set_num_threads(min(8, os.cpu_count() or 1))
        try:
            print(f"PyTorch threads: {torch.get_num_threads()}")
            print(f"RAM disponible: {psutil.virtual_memory().available / (1024**3):.1f} GB")
            if torch.cuda.is_available():
                print(f"CUDA disponible: {torch.cuda.get_device_name()}")
                print(f"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
        except Exception:
            pass

    def initialize_model(self):
        print(f"Initialisation du mod√®le {self.config.BASE_MODEL}...")

        # Tokenizer
        tokenizer_kwargs = {}
        if self.hf_token:
            tokenizer_kwargs['token'] = self.hf_token

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.BASE_MODEL,
            padding_side="left",
            **tokenizer_kwargs
        )

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # Tokens sp√©ciaux depuis la config si disponibles
        required_tokens = []
        try:
            special_map = getattr(self.config, 'HCP_SPECIAL_TOKENS', None)
            if isinstance(special_map, dict):
                required_tokens = list(special_map.values())
        except Exception:
            required_tokens = ["<|hcp|>", "<|territory|>", "<|indicator|>", "<|genre|>", "<|source|>"]

        # Ajouter tokens manquants
        add_tokens = [t for t in required_tokens if t and t not in self.tokenizer.get_vocab()]
        if add_tokens:
            self.tokenizer.add_special_tokens({'additional_special_tokens': add_tokens})
            print(f"Tokens sp√©ciaux ajout√©s: {add_tokens}")

        # Chargement mod√®le avec dtype adapt√©
        model_kwargs = {}
        if self.hf_token:
            model_kwargs['token'] = self.hf_token

        # Choix dtype
        use_fp16 = bool(getattr(self.config, 'FP16', False)) and torch.cuda.is_available()
        torch_dtype = torch.float16 if use_fp16 else torch.float32

        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.BASE_MODEL,
            torch_dtype=torch_dtype,
            low_cpu_mem_usage=True,
            **model_kwargs
        )

        # Resize embeddings si tokens ajout√©s
        if add_tokens:
            self.model.resize_token_embeddings(len(self.tokenizer))

        # D√©placer sur device si cuda
        if torch.cuda.is_available():
            try:
                self.model.to('cuda')
                print("Mod√®le d√©plac√© sur CUDA")
            except Exception as e:
                print(f"‚ö†Ô∏è Impossible de d√©placer le mod√®le sur CUDA: {e}")

        print(f"Mod√®le initialis√©: {self.config.BASE_MODEL}")

    def format_dialogue_for_training(self, data_item):
        """Formate les donn√©es pour l'entra√Ænement avec contexte HCP enrichi."""
        if 'conversation' in data_item and data_item['conversation']:
            return data_item['conversation']

        question = data_item.get('input_text', data_item.get('question', '')).strip()
        answer = data_item.get('target_text', data_item.get('answer', '')).strip()

        territory = data_item.get('territory', data_item.get('original_territory', ''))
        indicator = data_item.get('variable', data_item.get('indicateur', ''))
        genre = data_item.get('sexe', data_item.get('genre', ''))
        source = data_item.get('source_data', data_item.get('source', ''))

        context_parts = ['<|hcp|>']
        if territory and territory not in ['Territoire non sp√©cifi√©', '']:
            context_parts.append(f"<|territory|>{territory}")
        if indicator and indicator not in ['', 'indicateur_demographique']:
            context_parts.append(f"<|indicator|>{indicator}")
        if genre and genre not in ['', 'non sp√©cifi√©']:
            context_parts.append(f"<|genre|>{genre}")
        if source and source not in ['', 'unknown']:
            context_parts.append(f"<|source|>{source}")

        context_prefix = ''.join(context_parts)
        formatted = f"{context_prefix}<|user|>{question}<|assistant|>{answer}<|endoftext|>"
        return formatted

    def tokenize_data(self, examples):
        """Tokenisation sans multiprocessing pour √©viter les erreurs CUDA."""
        conversations = []
        length = len(examples['input_text'])
        for i in range(length):
            data_item = {
                'input_text': examples['input_text'][i],
                'target_text': examples['target_text'][i],
                'territory': (examples.get('territory') or [''])[i] if 'territory' in examples else '',
                'variable': (examples.get('variable') or [''])[i] if 'variable' in examples else '',
                'sexe': (examples.get('sexe') or [''])[i] if 'sexe' in examples else '',
                'source_data': (examples.get('source_data') or [''])[i] if 'source_data' in examples else ''
            }
            conversations.append(self.format_dialogue_for_training(data_item))

        inputs = self.tokenizer(
            conversations,
            truncation=True,
            padding='max_length',
            max_length=self.config.MAX_LENGTH,
            return_tensors=None
        )

        inputs['labels'] = inputs['input_ids'].copy()
        return inputs

    def prepare_dataset(self, training_data):
        """Pr√©paration du dataset sans multiprocessing."""
        print(f"Pr√©paration du dataset avec {len(training_data)} √©chantillons...")
        cleaned_data = []

        field_mapping = {
            'input_text': ['input_text', 'question'],
            'target_text': ['target_text', 'answer'],
            'territory': ['territory', 'original_territory', 'territoire'],
            'variable': ['variable', 'indicator', 'indicateur'],
            'sexe': ['sexe', 'genre'],
            'source_data': ['source_data', 'source'],
            'question_type': ['question_type'],
            'data_source': ['data_source']
        }

        for item in training_data:
            cleaned_item = {}
            for target_field, source_fields in field_mapping.items():
                val = None
                for sf in source_fields:
                    if sf in item and item[sf]:
                        val = item[sf]
                        break
                cleaned_item[target_field] = '' if val is None else str(val).strip()

            if cleaned_item['input_text'] and cleaned_item['target_text']:
                if cleaned_item.get('sexe') in ['', 'non sp√©cifi√©']:
                    cleaned_item['sexe'] = 'ensemble'
                if cleaned_item.get('territory') == '':
                    cleaned_item['territory'] = 'Territoire non sp√©cifi√©'
                cleaned_data.append(cleaned_item)

        if not cleaned_data:
            raise ValueError("Aucune donn√©e valide apr√®s nettoyage")

        dataset = Dataset.from_list(cleaned_data)

        # Tokenisation sans multiprocessing
        tokenized = dataset.map(
            self.tokenize_data,
            batched=True,
            num_proc=None,
            remove_columns=dataset.column_names,
            desc="Tokenisation"
        )

        return tokenized

    def create_stratified_split(self, training_data, train_ratio=0.7, val_ratio=0.3):
        """
        Cr√©e un split stratifi√© 70% train / 30% validation bas√© sur:
        - Territory (pour couvrir tous les territoires)
        - Indicator type (pour couvrir tous les types d'indicateurs)
        - Genre (pour √©quilibrer masculin/f√©minin/ensemble)
        """
        import random
        
        print(f"Cr√©ation du split stratifi√©: {train_ratio:.0%} train / {val_ratio:.0%} validation")
        
        # Grouper par (territoire, indicateur, genre) pour stratification
        strata = defaultdict(list)
        for i, item in enumerate(training_data):
            territory = item.get('territory', 'Unknown')
            indicator = item.get('variable', 'unknown')
            genre = item.get('sexe', 'ensemble')
            strata_key = f"{territory}_{indicator}_{genre}"
            strata[strata_key].append(i)
        
        print(f"Nombre de strates identifi√©es: {len(strata)}")
        
        train_indices = []
        val_indices = []
        
        for strata_key, indices in strata.items():
            random.shuffle(indices)
            # CORRECTION: calculer train en premier
            n_train = max(1, int(len(indices) * train_ratio))
            n_val = len(indices) - n_train
            
            train_indices.extend(indices[:n_train])
            val_indices.extend(indices[n_train:])
        
        # M√©langer les indices finaux
        random.shuffle(train_indices)
        random.shuffle(val_indices)
        
        train_data = [training_data[i] for i in train_indices]
        val_data = [training_data[i] for i in val_indices]
        
        print(f"Split cr√©√©:")
        print(f"  - Entra√Ænement: {len(train_data)} √©chantillons ({len(train_data)/len(training_data)*100:.1f}%)")
        print(f"  - Validation: {len(val_data)} √©chantillons ({len(val_data)/len(training_data)*100:.1f}%)")
        
        # V√©rification de la distribution
        self._verify_split_distribution(train_data, val_data)
        
        return train_data, val_data
        
    def _verify_split_distribution(self, train_data, val_data):
        """V√©rifie la distribution des donn√©es dans le split."""
        
        def get_distribution(data, key):
            dist = defaultdict(int)
            for item in data:
                dist[item.get(key, 'Unknown')] += 1
            return dist
        
        print("\nüìä V√©rification de la distribution:")
        
        # Distribution par genre
        train_genre = get_distribution(train_data, 'sexe')
        val_genre = get_distribution(val_data, 'sexe')
        print(f"Genre - Train: {dict(train_genre)}")
        print(f"Genre - Val: {dict(val_genre)}")
        
        # Distribution par source
        train_source = get_distribution(train_data, 'source_data')
        val_source = get_distribution(val_data, 'source_data')
        print(f"Sources - Train: {len(train_source)} types")
        print(f"Sources - Val: {len(val_source)} types")
        
        # Territoires uniques
        train_territories = set(item.get('territory', 'Unknown') for item in train_data)
        val_territories = set(item.get('territory', 'Unknown') for item in val_data)
        common_territories = train_territories.intersection(val_territories)
        
        print(f"Territoires - Train: {len(train_territories)} uniques")
        print(f"Territoires - Val: {len(val_territories)} uniques")
        print(f"Territoires communs: {len(common_territories)} ({len(common_territories)/len(train_territories)*100:.1f}%)")

    def calculate_compatible_steps(self, dataset_size, batch_size, num_epochs, use_validation=False):
        grad_acum = getattr(self.config, 'GRADIENT_ACCUMULATION_STEPS', 1)
        steps_per_epoch = max(1, dataset_size // (batch_size * grad_acum))
        if dataset_size % (batch_size * grad_acum) != 0:
            steps_per_epoch += 1
        total_steps = steps_per_epoch * num_epochs
        
        logging_steps = max(5, min(steps_per_epoch // 10, 50))
        
        if use_validation:
            eval_steps = max(10, steps_per_epoch // 3)
            # S'assurer que save_steps est un multiple de eval_steps
            save_steps = eval_steps
        else:
            eval_steps = None
            save_steps = min(steps_per_epoch // 2, 500)

        return {
            'total_steps': total_steps,
            'steps_per_epoch': steps_per_epoch,
            'logging_steps': logging_steps,
            'save_steps': save_steps,
            'eval_steps': eval_steps
        }

    def _validate_hcp_training_data(self, training_data):
        if not training_data:
            raise ValueError("Aucune donn√©e fournie")
        
        valid = 0
        for item in training_data[:200]:
            if isinstance(item, dict) and (item.get('question') or item.get('input_text')) and (item.get('answer') or item.get('target_text')):
                valid += 1
        
        if valid == 0:
            raise ValueError("Aucun item valide d√©tect√© dans les 200 premiers √©l√©ments")
        
        print(f"‚úÖ Validation r√©ussie: {valid}/200 √©chantillons valides test√©s")
        return True

    def save_hcp_enhanced_metadata(self, train_data, training_time, val_data=None, original_size=None):
        """Sauvegarde des m√©tadonn√©es d'entra√Ænement enrichies."""
        metadata = {
            'training_time_minutes': training_time,
            'original_dataset_size': original_size or len(train_data),
            'train_size': len(train_data),
            'validation_size': len(val_data) if val_data else 0,
            'model_config': {
                'base_model': self.config.BASE_MODEL,
                'max_length': self.config.MAX_LENGTH,
                'batch_size': self.config.BATCH_SIZE,
                'learning_rate': self.config.LEARNING_RATE,
                'num_epochs': self.config.NUM_EPOCHS,
                'fp16': getattr(self.config, 'FP16', False)
            },
            'data_distribution': {
                'territories_count': len(set(item.get('territory', 'Unknown') for item in train_data)),
                'indicators_count': len(set(item.get('variable', 'unknown') for item in train_data)),
                'sources_count': len(set(item.get('source_data', 'unknown') for item in train_data))
            }
        }
        
        os.makedirs(self.config.MODEL_PATH, exist_ok=True)
        path = os.path.join(self.config.MODEL_PATH, 'hcp_enhanced_metadata.json')
        
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"M√©tadonn√©es sauv√©es: {path}")
        return metadata

    def print_hcp_training_summary(self, training_data, training_time, original_size, val_data=None):
        print(f"\nüéØ R√âSUM√â D'ENTRA√éNEMENT HCP")
        print(f"{'='*50}")
        print(f"Temps d'entra√Ænement: {training_time:.1f} minutes")
        print(f"Dataset original: {original_size:,} √©chantillons")
        print(f"Entra√Ænement: {len(training_data):,} √©chantillons")
        if val_data:
            print(f"Validation: {len(val_data):,} √©chantillons")
        print(f"Mod√®le sauv√©: {self.config.MODEL_PATH}")
        print(f"{'='*50}")

    def train_model(self, training_data, train_ratio=0.7, val_ratio=0.3):
        """
        Entra√Æne le mod√®le avec un split train/validation automatique.
        
        Args:
            training_data: Liste des donn√©es d'entra√Ænement
            train_ratio: Ratio pour l'entra√Ænement (d√©faut: 0.7)
            val_ratio: Ratio pour la validation (d√©faut: 0.3)
        """
        start_time = time.time()
        original_size = len(training_data)
        
        print(f"üöÄ D√©but de l'entra√Ænement HCP avec {original_size:,} √©chantillons")
        
        if self.model is None:
            self.initialize_model()

        self._validate_hcp_training_data(training_data)

        # Cr√©er le split stratifi√©
        train_data, val_data = self.create_stratified_split(
            training_data, 
            train_ratio=train_ratio, 
            val_ratio=val_ratio
        )

        # Pr√©parer les datasets
        print("\nüîÑ Pr√©paration des datasets...")
        train_dataset = self.prepare_dataset(train_data)
        val_dataset = self.prepare_dataset(val_data) if val_data else None

        # Configuration des √©tapes d'entra√Ænement
        steps_cfg = self.calculate_compatible_steps(
            len(train_dataset), 
            self.config.BATCH_SIZE, 
            self.config.NUM_EPOCHS, 
            use_validation=bool(val_dataset)
        )

        print(f"\n‚öôÔ∏è Configuration d'entra√Ænement:")
        print(f"  - Steps par √©poque: {steps_cfg['steps_per_epoch']}")
        print(f"  - Steps total: {steps_cfg['total_steps']}")
        print(f"  - Logging steps: {steps_cfg['logging_steps']}")
        print(f"  - Save steps: {steps_cfg['save_steps']}")
        if steps_cfg['eval_steps']:
            print(f"  - Eval steps: {steps_cfg['eval_steps']}")

        # Arguments d'entra√Ænement
        training_args = TrainingArguments(
            output_dir=self.config.MODEL_PATH,
            overwrite_output_dir=True,
            num_train_epochs=self.config.NUM_EPOCHS,
            per_device_train_batch_size=self.config.BATCH_SIZE,
            per_device_eval_batch_size=self.config.BATCH_SIZE,
            gradient_accumulation_steps=getattr(self.config, 'GRADIENT_ACCUMULATION_STEPS', 1),
            learning_rate=self.config.LEARNING_RATE,
            weight_decay=getattr(self.config, 'WEIGHT_DECAY', 0.01),
            warmup_steps=max(100, steps_cfg['total_steps'] // 20),
            dataloader_num_workers=0,
            remove_unused_columns=False,
            logging_steps=steps_cfg['logging_steps'],
            save_steps=steps_cfg['save_steps'],
            save_total_limit=3,
            evaluation_strategy="steps" if steps_cfg['eval_steps'] else "no",
            eval_steps=steps_cfg['eval_steps'],
            fp16=bool(getattr(self.config, 'FP16', False)) and torch.cuda.is_available(),
            report_to=None,
            load_best_model_at_end=False,  # D√©sactiv√© pour √©viter les probl√®mes
            gradient_checkpointing=True,
            dataloader_pin_memory=False,
            seed=42
        )

        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer, 
            mlm=False
        )

        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=data_collator
        )

        try:
            print(f"\nüéØ D√©marrage de l'entra√Ænement...")
            trainer.train()
            
            print(f"\nüíæ Sauvegarde du mod√®le...")
            trainer.save_model()
            self.tokenizer.save_pretrained(self.config.MODEL_PATH)
            
            elapsed_time = (time.time() - start_time) / 60
            
            # Sauvegarder les m√©tadonn√©es
            self.save_hcp_enhanced_metadata(train_data, elapsed_time, val_data, original_size)
            
            # Afficher le r√©sum√©
            self.print_hcp_training_summary(train_data, elapsed_time, original_size, val_data)
            
            print(f"‚úÖ Entra√Ænement termin√© avec succ√®s!")
            
        except RuntimeError as e:
            print(f"‚ùå Erreur d'entra√Ænement (RuntimeError): {e}")
            if 'out of memory' in str(e).lower():
                print("üí° Suggestions pour r√©soudre l'erreur OOM:")
                print("  - R√©duire BATCH_SIZE dans config.py")
                print("  - R√©duire MAX_LENGTH dans config.py")
                print("  - Activer gradient_checkpointing (d√©j√† activ√©)")
                print("  - Utiliser un mod√®le plus petit")
            raise
        except Exception as e:
            print(f"‚ùå Erreur inattendue: {e}")
            raise
        finally:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def get_model_info(self):
        """Retourne des informations sur le mod√®le entra√Æn√©."""
        if self.model is None:
            return {"status": "Model not initialized"}
        
        return {
            "model_name": self.config.BASE_MODEL,
            "vocab_size": len(self.tokenizer),
            "special_tokens": len(self.tokenizer.additional_special_tokens),
            "model_parameters": sum(p.numel() for p in self.model.parameters()),
            "trainable_parameters": sum(p.numel() for p in self.model.parameters() if p.requires_grad),
            "model_size_mb": sum(p.numel() * p.element_size() for p in self.model.parameters()) / (1024*1024),
            "device": next(self.model.parameters()).device if self.model else "unknown"
        }


# Fonction utilitaire pour l'entra√Ænement facile
def train_hcp_model_with_split(config, training_data, train_ratio=0.7, val_ratio=0.3):
    """
    Fonction helper pour entra√Æner facilement un mod√®le HCP avec split automatique.
    
    Args:
        config: Configuration HCP
        training_data: Donn√©es d'entra√Ænement
        train_ratio: Ratio d'entra√Ænement (d√©faut: 70%)
        val_ratio: Ratio de validation (d√©faut: 30%)
    
    Returns:
        HCPChatbotTrainer: Instance du trainer avec mod√®le entra√Æn√©
    """
    trainer = HCPChatbotTrainer(config)
    trainer.train_model(training_data, train_ratio=train_ratio, val_ratio=val_ratio)
    
    print(f"\nüìä Informations sur le mod√®le entra√Æn√©:")
    model_info = trainer.get_model_info()
    for key, value in model_info.items():
        print(f"  {key}: {value}")
    
    return trainer


def check_system_resources_hcp(min_ram_gb: float = 8.0):
    """V√©rifie les ressources syst√®me pour l'entra√Ænement HCP."""
    memory = psutil.virtual_memory()
    cpu = psutil.cpu_percent(interval=1)
    
    available_gb = memory.available / (1024**3)
    ready = available_gb >= min_ram_gb
    
    cuda_info = {}
    if torch.cuda.is_available():
        cuda_info = {
            "cuda_available": True,
            "device_name": torch.cuda.get_device_name(),
            "cuda_memory_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3),
            "cuda_memory_allocated": torch.cuda.memory_allocated() / (1024**3),
            "cuda_memory_cached": torch.cuda.memory_reserved() / (1024**3)
        }
    else:
        cuda_info = {"cuda_available": False}
    
    return {
        'ready': ready,
        'memory_available_gb': available_gb,
        'memory_total_gb': memory.total / (1024**3),
        'cpu_usage_percent': cpu,
        'cpu_count': os.cpu_count(),
        **cuda_info
    }


# Script de test
if __name__ == "__main__":
    print("üß™ Test du trainer HCP corrig√©\n")
    
    # V√©rifier les ressources
    resources = check_system_resources_hcp()
    print(f"üíª Ressources syst√®me:")
    for key, value in resources.items():
        print(f"  {key}: {value}")
    
    if not resources['ready']:
        print(f"‚ö†Ô∏è RAM insuffisante (besoin: 8GB, disponible: {resources['memory_available_gb']:.1f}GB)")
    else:
        print(f"‚úÖ Ressources suffisantes pour l'entra√Ænement")